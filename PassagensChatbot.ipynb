{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load data"
      ],
      "metadata": {
        "id": "MZ22pBRX98Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "8u16EHBM-rtV",
        "outputId": "03007d60-6817-45d4-c3c1-07e4824ec120"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb6d8c2c-1d0a-449a-8335-5fa3e85d4fb9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cb6d8c2c-1d0a-449a-8335-5fa3e85d4fb9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dialogues_018.json to dialogues_018.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# intents\n",
        "!mkdir -p dialogue_018/NONE\n",
        "!mkdir -p dialogue_018/SearchOnewayFlight\n",
        "!mkdir -p dialogue_018/ReserveOnewayFlight\n",
        "!mkdir -p dialogue_018/SearchRoundtripFlights\n",
        "!mkdir -p dialogue_018/ReserveRoundtripFlights"
      ],
      "metadata": {
        "id": "EVVnuFYREycJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script para coletar apenas as utterances (frases) do usuário e armazená-las num diretório que representa a intenção(que será nossa classe para a tarefa de classificação de intenção).</br>\n",
        "<b>atenção</b>: O sistema foi excluído da coleta de utterances porque a resposta do sistema não possui intenção"
      ],
      "metadata": {
        "id": "mPzqHs88CbQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"dialogues_018.json\", 'r') as file:\n",
        "        data_018 = json.load(file)\n",
        "\n",
        "    for i in range(len(data_018)):\n",
        "        frames_amount = len(data_018[i]['turns'])\n",
        "        for j in range(frames_amount):\n",
        "            current_frame = data_018[i]['turns'][j]\n",
        "            speaker = current_frame['speaker']\n",
        "            if speaker == 'USER':\n",
        "                intent = current_frame['frames'][0]['state']['active_intent']\n",
        "                utterance = current_frame['utterance']\n",
        "\n",
        "                file_path = \"dialogue_018/\" + intent + \"/\" + str(i) + \"_\" + str(j) + \".txt\"\n",
        "\n",
        "                f = open(file_path, \"w+\")\n",
        "                f.write(utterance)\n",
        "                f.close()\n"
      ],
      "metadata": {
        "id": "7mD2KTnt-Rg8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Armazenar numa estrutura para treinamento no Keras"
      ],
      "metadata": {
        "id": "zAPqnY0WDLev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds= tf.keras.utils.text_dataset_from_directory(\n",
        "    '/content/dialogue_018',\n",
        "    labels='inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed,\n",
        "    shuffle=False)\n",
        "\n",
        "raw_validation_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    '/content/dialogue_018',\n",
        "    labels='inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed,\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPmrl8B1TrRt",
        "outputId": "75c8c9e2-92eb-4221-de7d-a354e974ff08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1052 files belonging to 5 classes.\n",
            "Using 842 files for training.\n",
            "Found 1052 files belonging to 5 classes.\n",
            "Using 210 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFgAwDctR5W",
        "outputId": "1424d1c9-9134-4356-c113-af6c0a4e8a91"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NONE',\n",
              " 'ReserveOnewayFlight',\n",
              " 'ReserveRoundtripFlights',\n",
              " 'SearchOnewayFlight',\n",
              " 'SearchRoundtripFlights']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in raw_train_ds:\n",
        "  print(\"Utterance: \", text_batch)\n",
        "  print(\"Label: \", label_batch)"
      ],
      "metadata": {
        "id": "xZMI9c_kFS5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV7HzrCaIyWn",
        "outputId": "7ed94876-df0a-40c9-a4a8-f24b30d92983"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acho que o Tensor acima é dividido em batches e acredito que isso adiciona uma complexidade na hora de tokenizar, pois teríamos que fazer para cada um dos tensores. Testei resolver fazer uma concatenação de todos os tensores abaixo:"
      ],
      "metadata": {
        "id": "nrVsXgbEJjjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_tensor = tf.concat([text_batch for text_batch, label_batch in raw_train_ds], axis=0)\n",
        "#all_text_tensor"
      ],
      "metadata": {
        "id": "1o5cP6L6JLBx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = tf.concat([label_batch for text_batch, label_batch in raw_train_ds], axis=0)\n",
        "#all_labels"
      ],
      "metadata": {
        "id": "9Htct7EgLYae"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utterances_validation_tensor = tf.concat([text_batch for text_batch, label_batch in raw_validation_ds], axis=0)"
      ],
      "metadata": {
        "id": "S0j0NKdNMA-9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_labels = tf.concat([label_batch for text_batch, label_batch in raw_validation_ds], axis=0)"
      ],
      "metadata": {
        "id": "zeozrI7tMGi3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre processamento de dados\n",
        "Fazer uma bag-of-words"
      ],
      "metadata": {
        "id": "Ug1zDO2rMFLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import json\n",
        "import numpy"
      ],
      "metadata": {
        "id": "v7s5l7y6WWlB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text, sequence"
      ],
      "metadata": {
        "id": "jxHrOBaChxa8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train subset"
      ],
      "metadata": {
        "id": "OZj_2SooMm8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "texts = all_text_tensor.numpy().astype(str)\n",
        "t.fit_on_texts(texts)\n",
        "\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(texts, mode='count')\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWpBBYtYWdxe",
        "outputId": "5f982c8f-efdd-4ae2-b85b-e23ddbeebc6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('no', 73), ('that', 175), ('will', 62), ('be', 70), ('all', 75), ('thank', 75), ('you', 191), (\"that's\", 68), ('thanks', 92), ('for', 217), ('your', 54), ('help', 101), ('nope', 8), (\"we're\", 1), ('good', 101), ('a', 93), ('lot', 5), ('the', 280), ('is', 114), (\"i'm\", 46), ('right', 17), ('though', 1), ('thats', 8), ('it', 54), ('i', 282), ('just', 8), ('needed', 8), ('information', 2), ('now', 11), ('again', 4), ('everything', 13), ('would', 50), ('very', 10), ('much', 16), ('do', 17), ('not', 5), ('want', 54), ('yet', 1), ('was', 1), ('okay', 11), ('this', 63), ('should', 1), ('enough', 1), ('to', 225), ('know', 4), ('maybe', 1), ('another', 6), ('time', 24), (\"that'll\", 1), ('wanted', 2), ('but', 6), ('am', 62), ('nothing', 1), ('else', 7), ('set', 1), ('yes', 122), ('can', 93), ('assist', 3), ('me', 145), ('with', 14), ('buying', 3), ('ticket', 67), ('correct', 20), ('yeah', 3), ('perfect', 24), ('great', 34), ('sure', 2), ('go', 16), ('ahead', 4), ('and', 125), ('buy', 29), ('please', 70), ('get', 18), ('economy', 42), ('class', 14), ('tickets', 67), ('need', 51), ('three', 8), ('actually', 4), ('only', 11), ('2', 6), ('when', 36), ('does', 81), ('flight', 166), ('end', 1), ('at', 17), ('which', 35), ('airport', 51), ('plan', 9), ('book', 57), ('one', 25), ('way', 10), ('trip', 60), ('take', 2), ('so', 13), ('sounds', 109), ('travel', 12), ('soon', 5), ('oneway', 3), ('leave', 44), ('from', 103), ('what', 45), ('arrive', 57), ('refundable', 35), ('like', 53), ('could', 20), ('flying', 10), ('my', 39), ('plans', 3), ('might', 1), ('change', 2), ('purchase', 6), ('onward', 15), (\"it's\", 5), ('short', 28), ('both', 2), ('flights', 79), ('yep', 2), ('into', 4), ('ok', 8), ('them', 1), ('refund', 4), ('works', 10), ('3', 8), ('people', 25), ('details', 2), ('are', 43), ('helping', 5), ('alright', 2), ('since', 2), ('planning', 11), ('on', 153), ('making', 1), ('in', 39), ('confirm', 1), ('where', 9), ('departure', 3), ('these', 1), ('tell', 8), ('our', 1), ('destination', 4), ('if', 8), ('person', 11), ('may', 3), ('there', 43), ('together', 1), ('make', 2), ('1', 13), ('4', 7), ('fly', 8), ('out', 6), ('of', 81), ('today', 5), (\"it'll\", 1), (\"i'd\", 12), ('purchasing', 2), ('depart', 14), ('also', 5), ('arrival', 13), ('appreciate', 1), ('reserve', 5), ('going', 37), ('behalf', 1), ('refunded', 2), ('especially', 1), ('return', 50), ('arriving', 3), ('assistance', 7), ('appreciated', 2), ('has', 3), ('stop', 4), ('group', 15), ('prefer', 15), ('premium', 6), ('im', 2), ('yup', 1), (\"you're\", 1), ('really', 2), ('big', 1), ('oh', 1), ('nice', 1), ('landing', 1), ('travelling', 2), ('airports', 3), (\"you've\", 1), ('been', 1), ('helpful', 1), ('chicago', 8), ('kindly', 1), ('persons', 3), ('whether', 1), ('0', 3), ('stops', 16), ('starts', 1), ('choice', 2), ('outbound', 2), ('arrives', 2), ('seattle', 25), ('four', 5), (\"i'll\", 14), ('gone', 1), ('little', 1), ('while', 1), ('sorry', 2), ('let', 1), ('see', 1), ('available', 21), ('exactly', 1), ('because', 2), ('either', 2), ('business', 5), ('or', 8), ('fine', 4), ('think', 2), ('its', 1), ('departing', 14), ('as', 4), ('well', 2), ('small', 1), (\"there's\", 1), ('mean', 1), ('we', 2), ('upon', 1), ('how', 7), ('many', 6), ('have', 19), ('incorrect', 1), ('total', 1), ('returning', 10), ('excellent', 1), ('corrct', 1), ('any', 28), ('outgoing', 1), ('atlanta', 13), ('two', 7), ('work', 5), ('an', 4), ('wonderful', 1), ('best', 1), ('airline', 6), ('leaving', 62), ('san', 14), ('francisco', 7), ('los', 5), ('angeles', 5), ('find', 39), ('trio', 1), ('matches', 1), ('needs', 1), ('done', 1), ('names', 1), ('shift', 1), ('anyway', 1), ('including', 1), ('myself', 1), ('seats', 1), ('us', 1), ('offer', 1), ('require', 1), ('enjoy', 1), ('traveling', 23), ('times', 3), ('purchases', 1), ('able', 2), ('next', 9), ('friday', 1), ('nyc', 3), ('wa', 10), ('delta', 9), ('airlines', 24), ('paris', 4), ('france', 2), ('14th', 12), ('march', 73), ('ciudad', 4), ('de', 4), ('mexico', 5), ('diego', 4), ('upcoming', 1), ('sf', 9), ('toronto', 2), ('canada', 2), ('4th', 9), ('month', 27), ('other', 26), ('option', 1), ('options', 5), ('vacation', 4), ('washington', 3), ('10th', 13), ('cdmx', 2), ('seat', 1), ('coming', 7), ('up', 5), ('12th', 20), ('london', 8), ('ga', 7), ('2nd', 12), ('heading', 2), ('ny', 4), ('hey', 2), ('looking', 6), ('3rd', 10), ('sd', 3), ('portland', 7), ('8th', 13), ('some', 15), ('las', 4), ('vegas', 6), (\"don't\", 2), ('care', 2), ('about', 5), ('anything', 4), ('phoenix', 8), ('something', 3), ('new', 5), ('york', 5), ('traveing', 1), ('saturday', 4), ('week', 6), ('zero', 5), ('come', 9), ('back', 29), ('search', 22), ('united', 6), ('7th', 3), ('11th', 10), ('lax', 4), ('5th', 9), ('look', 8), ('6th', 6), ('use', 2), ('chi', 8), ('town', 8), ('visiting', 2), ('last', 1), ('offered', 1), ('9th', 11), ('regret', 1), ('sunday', 1), ('vancouver', 4), ('thursday', 3), ('az', 3), ('american', 7), ('la', 11), ('city', 3), ('land', 1), ('13th', 4), ('england', 2), ('fight', 2), ('possible', 1), ('retun', 1), ('fran', 3), ('1st', 3), ('philadelphia', 2), ('southwest', 2), ('round', 26), ('philly', 1), ('tomorrow', 4), ('they', 1), ('interested', 2), ('holiday', 1), ('thinking', 3), ('atl', 4), ('wow', 1), ('scan', 1), ('monday', 4), ('trips', 1), (\"what's\", 1), ('hometown', 1), ('attend', 1), ('family', 1), ('gathering', 1), ('sfo', 4), ('uk', 1), ('conference', 1), ('seating', 1), (\"doesn't\", 3), ('matter', 2), ('moving', 2), ('tuesday', 1), ('suggest', 1), ('arive', 1), ('number', 1), ('others', 2), ('initial', 1), ('hello', 1), ('getting', 1), ('list', 1), ('accept', 1), ('sound', 1), ('later', 1), ('too', 1), ('station', 1), ('through', 2), ('more', 1), ('than', 1), ('alternative', 1), ('m', 1), ('date', 1), ('schedules', 1), ('locations', 1), ('looks', 1), ('via', 1), ('mind', 1), ('searching', 1), ('form', 1), ('visit', 1), ('specifically', 1), ('delhi', 1)])\n",
            "842\n",
            "{'i': 1, 'the': 2, 'to': 3, 'for': 4, 'you': 5, 'that': 6, 'flight': 7, 'on': 8, 'me': 9, 'and': 10, 'yes': 11, 'is': 12, 'sounds': 13, 'from': 14, 'help': 15, 'good': 16, 'a': 17, 'can': 18, 'thanks': 19, 'does': 20, 'of': 21, 'flights': 22, 'all': 23, 'thank': 24, 'no': 25, 'march': 26, 'be': 27, 'please': 28, \"that's\": 29, 'ticket': 30, 'tickets': 31, 'this': 32, 'will': 33, 'am': 34, 'leaving': 35, 'trip': 36, 'book': 37, 'arrive': 38, 'your': 39, 'it': 40, 'want': 41, 'like': 42, 'need': 43, 'airport': 44, 'would': 45, 'return': 46, \"i'm\": 47, 'what': 48, 'leave': 49, 'are': 50, 'there': 51, 'economy': 52, 'my': 53, 'in': 54, 'find': 55, 'going': 56, 'when': 57, 'which': 58, 'refundable': 59, 'great': 60, 'buy': 61, 'back': 62, 'short': 63, 'any': 64, 'month': 65, 'other': 66, 'round': 67, 'one': 68, 'people': 69, 'seattle': 70, 'time': 71, 'perfect': 72, 'airlines': 73, 'traveling': 74, 'search': 75, 'available': 76, 'correct': 77, 'could': 78, '12th': 79, 'have': 80, 'get': 81, 'right': 82, 'do': 83, 'at': 84, 'much': 85, 'go': 86, 'stops': 87, 'onward': 88, 'group': 89, 'prefer': 90, 'some': 91, 'with': 92, 'class': 93, 'depart': 94, \"i'll\": 95, 'departing': 96, 'san': 97, 'everything': 98, 'so': 99, '1': 100, 'arrival': 101, 'atlanta': 102, '10th': 103, '8th': 104, 'travel': 105, \"i'd\": 106, '14th': 107, '2nd': 108, 'now': 109, 'okay': 110, 'only': 111, 'planning': 112, 'person': 113, '9th': 114, 'la': 115, 'very': 116, 'way': 117, 'flying': 118, 'works': 119, 'returning': 120, 'wa': 121, '3rd': 122, '11th': 123, 'plan': 124, 'where': 125, 'next': 126, 'delta': 127, 'sf': 128, '4th': 129, 'come': 130, '5th': 131, 'nope': 132, 'thats': 133, 'just': 134, 'needed': 135, 'three': 136, 'ok': 137, '3': 138, 'tell': 139, 'if': 140, 'fly': 141, 'chicago': 142, 'or': 143, 'london': 144, 'phoenix': 145, 'look': 146, 'chi': 147, 'town': 148, 'else': 149, '4': 150, 'assistance': 151, 'how': 152, 'two': 153, 'francisco': 154, 'coming': 155, 'ga': 156, 'portland': 157, 'american': 158, 'another': 159, 'but': 160, '2': 161, 'purchase': 162, 'out': 163, 'premium': 164, 'many': 165, 'airline': 166, 'looking': 167, 'vegas': 168, 'week': 169, 'united': 170, '6th': 171, 'lot': 172, 'not': 173, 'soon': 174, \"it's\": 175, 'helping': 176, 'today': 177, 'also': 178, 'reserve': 179, 'four': 180, 'business': 181, 'work': 182, 'los': 183, 'angeles': 184, 'mexico': 185, 'options': 186, 'up': 187, 'about': 188, 'new': 189, 'york': 190, 'zero': 191, 'again': 192, 'know': 193, 'ahead': 194, 'actually': 195, 'into': 196, 'refund': 197, 'destination': 198, 'stop': 199, 'fine': 200, 'as': 201, 'an': 202, 'paris': 203, 'ciudad': 204, 'de': 205, 'diego': 206, 'vacation': 207, 'ny': 208, 'las': 209, 'anything': 210, 'saturday': 211, 'lax': 212, 'vancouver': 213, '13th': 214, 'tomorrow': 215, 'atl': 216, 'monday': 217, 'sfo': 218, 'assist': 219, 'buying': 220, 'yeah': 221, 'oneway': 222, 'plans': 223, 'departure': 224, 'may': 225, 'arriving': 226, 'has': 227, 'airports': 228, 'persons': 229, '0': 230, 'times': 231, 'nyc': 232, 'washington': 233, 'sd': 234, 'something': 235, '7th': 236, 'thursday': 237, 'az': 238, 'city': 239, 'fran': 240, '1st': 241, 'thinking': 242, \"doesn't\": 243, 'information': 244, 'wanted': 245, 'sure': 246, 'take': 247, 'change': 248, 'both': 249, 'yep': 250, 'details': 251, 'alright': 252, 'since': 253, 'make': 254, 'purchasing': 255, 'refunded': 256, 'appreciated': 257, 'im': 258, 'really': 259, 'travelling': 260, 'choice': 261, 'outbound': 262, 'arrives': 263, 'sorry': 264, 'because': 265, 'either': 266, 'think': 267, 'well': 268, 'we': 269, 'able': 270, 'france': 271, 'toronto': 272, 'canada': 273, 'cdmx': 274, 'heading': 275, 'hey': 276, \"don't\": 277, 'care': 278, 'use': 279, 'visiting': 280, 'england': 281, 'fight': 282, 'philadelphia': 283, 'southwest': 284, 'interested': 285, 'matter': 286, 'moving': 287, 'others': 288, 'through': 289, \"we're\": 290, 'though': 291, 'yet': 292, 'was': 293, 'should': 294, 'enough': 295, 'maybe': 296, \"that'll\": 297, 'nothing': 298, 'set': 299, 'end': 300, 'might': 301, 'them': 302, 'making': 303, 'confirm': 304, 'these': 305, 'our': 306, 'together': 307, \"it'll\": 308, 'appreciate': 309, 'behalf': 310, 'especially': 311, 'yup': 312, \"you're\": 313, 'big': 314, 'oh': 315, 'nice': 316, 'landing': 317, \"you've\": 318, 'been': 319, 'helpful': 320, 'kindly': 321, 'whether': 322, 'starts': 323, 'gone': 324, 'little': 325, 'while': 326, 'let': 327, 'see': 328, 'exactly': 329, 'its': 330, 'small': 331, \"there's\": 332, 'mean': 333, 'upon': 334, 'incorrect': 335, 'total': 336, 'excellent': 337, 'corrct': 338, 'outgoing': 339, 'wonderful': 340, 'best': 341, 'trio': 342, 'matches': 343, 'needs': 344, 'done': 345, 'names': 346, 'shift': 347, 'anyway': 348, 'including': 349, 'myself': 350, 'seats': 351, 'us': 352, 'offer': 353, 'require': 354, 'enjoy': 355, 'purchases': 356, 'friday': 357, 'upcoming': 358, 'option': 359, 'seat': 360, 'traveing': 361, 'last': 362, 'offered': 363, 'regret': 364, 'sunday': 365, 'land': 366, 'possible': 367, 'retun': 368, 'philly': 369, 'they': 370, 'holiday': 371, 'wow': 372, 'scan': 373, 'trips': 374, \"what's\": 375, 'hometown': 376, 'attend': 377, 'family': 378, 'gathering': 379, 'uk': 380, 'conference': 381, 'seating': 382, 'tuesday': 383, 'suggest': 384, 'arive': 385, 'number': 386, 'initial': 387, 'hello': 388, 'getting': 389, 'list': 390, 'accept': 391, 'sound': 392, 'later': 393, 'too': 394, 'station': 395, 'more': 396, 'than': 397, 'alternative': 398, 'm': 399, 'date': 400, 'schedules': 401, 'locations': 402, 'looks': 403, 'via': 404, 'mind': 405, 'searching': 406, 'form': 407, 'visit': 408, 'specifically': 409, 'delhi': 410}\n",
            "defaultdict(<class 'int'>, {'no': 73, 'will': 61, 'all': 72, 'be': 68, 'that': 170, 'thank': 75, 'you': 190, 'your': 54, \"that's\": 68, 'thanks': 92, 'help': 101, 'for': 198, 'nope': 8, 'the': 240, \"we're\": 1, 'good': 101, 'a': 90, 'lot': 5, 'is': 106, \"i'm\": 44, 'right': 17, 'though': 1, 'it': 51, 'thats': 8, 'just': 8, 'i': 248, 'needed': 8, 'information': 2, 'now': 11, 'again': 4, 'everything': 13, 'would': 48, 'very': 10, 'much': 16, 'yet': 1, 'do': 16, 'want': 51, 'not': 5, 'was': 1, 'okay': 9, 'this': 60, 'should': 1, 'enough': 1, 'to': 186, 'know': 3, 'maybe': 1, 'time': 24, 'another': 5, \"that'll\": 1, 'wanted': 2, 'but': 6, 'am': 61, 'else': 7, 'nothing': 1, 'set': 1, 'assist': 3, 'with': 14, 'ticket': 67, 'buying': 3, 'me': 134, 'yes': 122, 'can': 93, 'correct': 20, 'yeah': 3, 'perfect': 24, 'great': 34, 'ahead': 4, 'go': 16, 'and': 118, 'sure': 2, 'buy': 29, 'class': 14, 'tickets': 67, 'get': 18, 'economy': 42, 'please': 69, 'need': 51, 'three': 8, '2': 6, 'only': 11, 'actually': 4, 'does': 74, 'airport': 51, 'at': 17, 'flight': 159, 'which': 35, 'when': 36, 'end': 1, 'plan': 9, 'book': 56, 'take': 2, 'way': 10, 'trip': 57, 'one': 25, 'so': 13, 'oneway': 3, 'sounds': 109, 'travel': 12, 'soon': 5, 'leave': 43, 'from': 103, 'what': 43, 'refundable': 35, 'arrive': 57, 'like': 52, 'could': 20, 'flying': 10, 'might': 1, 'change': 2, 'plans': 3, 'my': 39, 'purchase': 6, 'onward': 15, 'short': 28, \"it's\": 5, 'flights': 79, 'both': 2, 'yep': 2, 'into': 4, 'ok': 8, 'them': 1, 'refund': 4, '3': 8, 'people': 25, 'works': 10, 'details': 2, 'are': 42, 'helping': 5, 'alright': 2, 'since': 2, 'planning': 11, 'on': 120, 'making': 1, 'in': 39, 'where': 9, 'confirm': 1, 'departure': 3, 'these': 1, 'tell': 8, 'our': 1, 'destination': 4, 'if': 7, 'may': 3, 'person': 11, 'together': 1, 'there': 40, '1': 13, 'make': 2, '4': 7, 'of': 72, 'fly': 8, 'out': 6, 'today': 5, 'purchasing': 2, \"it'll\": 1, \"i'd\": 12, 'depart': 14, 'also': 5, 'arrival': 13, 'appreciate': 1, 'reserve': 5, 'going': 37, 'behalf': 1, 'refunded': 2, 'especially': 1, 'arriving': 3, 'return': 50, 'assistance': 7, 'appreciated': 2, 'has': 3, 'stop': 4, 'group': 15, 'premium': 6, 'prefer': 15, 'im': 2, 'yup': 1, 'really': 2, 'big': 1, \"you're\": 1, 'nice': 1, 'oh': 1, 'landing': 1, 'travelling': 2, 'airports': 3, 'helpful': 1, 'been': 1, \"you've\": 1, 'chicago': 8, 'persons': 3, 'kindly': 1, 'whether': 1, '0': 3, 'stops': 16, 'starts': 1, 'choice': 2, 'arrives': 2, 'outbound': 2, 'seattle': 25, 'four': 5, 'little': 1, \"i'll\": 12, 'gone': 1, 'while': 1, 'sorry': 2, 'let': 1, 'see': 1, 'available': 21, 'exactly': 1, 'fine': 4, 'because': 2, 'or': 8, 'either': 2, 'business': 5, 'its': 1, 'think': 2, 'well': 2, 'as': 3, 'departing': 14, 'small': 1, \"there's\": 1, 'mean': 1, 'we': 2, 'upon': 1, 'have': 19, 'how': 7, 'many': 6, 'incorrect': 1, 'total': 1, 'returning': 10, 'excellent': 1, 'corrct': 1, 'any': 28, 'outgoing': 1, 'atlanta': 13, 'two': 7, 'work': 5, 'an': 4, 'wonderful': 1, 'best': 1, 'airline': 6, 'leaving': 62, 'san': 14, 'francisco': 7, 'angeles': 5, 'los': 5, 'find': 39, 'trio': 1, 'needs': 1, 'matches': 1, 'done': 1, 'names': 1, 'shift': 1, 'myself': 1, 'including': 1, 'anyway': 1, 'seats': 1, 'us': 1, 'offer': 1, 'require': 1, 'traveling': 23, 'enjoy': 1, 'times': 3, 'purchases': 1, 'able': 2, 'next': 9, 'friday': 1, 'delta': 9, 'wa': 10, 'nyc': 3, 'airlines': 24, 'paris': 4, 'france': 2, 'march': 63, '14th': 12, 'ciudad': 4, 'diego': 4, 'mexico': 5, 'de': 4, 'upcoming': 1, 'sf': 9, '4th': 9, 'canada': 2, 'toronto': 2, 'month': 25, 'other': 26, 'option': 1, 'options': 5, 'vacation': 4, 'cdmx': 2, 'washington': 3, '10th': 13, 'seat': 1, 'up': 5, 'coming': 7, '12th': 20, 'london': 8, 'ga': 7, '2nd': 12, 'ny': 4, 'heading': 2, 'looking': 6, '3rd': 10, 'hey': 2, 'sd': 3, 'portland': 7, '8th': 13, 'some': 15, 'las': 4, 'vegas': 6, 'about': 5, 'care': 2, \"don't\": 2, 'anything': 4, 'something': 3, 'phoenix': 8, 'new': 5, 'york': 5, 'traveing': 1, 'week': 6, 'saturday': 4, 'zero': 5, 'back': 29, 'come': 9, 'search': 22, 'united': 6, '7th': 3, '11th': 10, '5th': 9, 'lax': 4, 'look': 8, '6th': 6, 'town': 8, 'chi': 8, 'use': 2, 'visiting': 2, 'offered': 1, 'last': 1, '9th': 11, 'regret': 1, 'sunday': 1, 'vancouver': 4, 'thursday': 3, 'az': 3, 'american': 7, 'la': 11, 'city': 3, 'land': 1, '13th': 4, 'england': 2, 'fight': 2, 'possible': 1, 'retun': 1, 'fran': 3, '1st': 3, 'philadelphia': 2, 'southwest': 2, 'round': 26, 'philly': 1, 'tomorrow': 4, 'they': 1, 'interested': 2, 'holiday': 1, 'thinking': 3, 'atl': 4, 'wow': 1, 'scan': 1, 'monday': 4, 'trips': 1, \"what's\": 1, 'hometown': 1, 'attend': 1, 'gathering': 1, 'family': 1, 'sfo': 4, 'uk': 1, 'conference': 1, 'matter': 2, 'seating': 1, \"doesn't\": 3, 'moving': 2, 'tuesday': 1, 'suggest': 1, 'arive': 1, 'number': 1, 'others': 2, 'initial': 1, 'hello': 1, 'getting': 1, 'list': 1, 'accept': 1, 'sound': 1, 'later': 1, 'too': 1, 'station': 1, 'through': 2, 'than': 1, 'more': 1, 'alternative': 1, 'm': 1, 'date': 1, 'locations': 1, 'schedules': 1, 'looks': 1, 'via': 1, 'searching': 1, 'mind': 1, 'form': 1, 'visit': 1, 'specifically': 1, 'delhi': 1})\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 2. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_token = t.texts_to_sequences(texts)\n",
        "padded_sequences = sequence.pad_sequences(seq_token)"
      ],
      "metadata": {
        "id": "HpdgcAJ2hTvw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU7wKRXIh_XI",
        "outputId": "e8158afc-1b56-4666-8967-a490f690af5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(842, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do62_JxXN6s3",
        "outputId": "fa56039f-590e-4290-96be-a6262222b02e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([842])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation subset"
      ],
      "metadata": {
        "id": "8EYJ9PZOMq-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = Tokenizer()\n",
        "validation_texts = utterances_validation_tensor.numpy().astype(str)\n",
        "t2.fit_on_texts(validation_texts)\n",
        "validation_seq_token = t2.texts_to_sequences(validation_texts)\n",
        "padded_validation = sequence.pad_sequences(validation_seq_token)\n",
        "#padded_validation = sequence.pad_sequences(validation_seq_token, maxlen=30)"
      ],
      "metadata": {
        "id": "xWtIC13BMtHB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_validation.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlKhpjirNkUA",
        "outputId": "2c8489e8-e075-4617-8c16-2254db8146d9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Xna25DNty1",
        "outputId": "9eb6e324-fa1a-485a-8a2d-41c4bb9af139"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([210])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de intenções (Parte 1)\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM </br>\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/ </br>"
      ],
      "metadata": {
        "id": "Vrd0nYb4IDGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "cHp8IuBtIqbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM\n",
        "from keras.layers import Embedding\n",
        "import keras.backend as K\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=842, output_dim=100))\n",
        "model.add(LSTM(units=64))\n",
        "model.add(Dense(5,activation='softmax'))\n",
        "model.compile(loss=['sparse_categorical_crossentropy'] , optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG-26GAEZWk8",
        "outputId": "eee912da-5955-4703-bfd7-bc6c30802d91"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 100)         84200     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                42240     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 126765 (495.18 KB)\n",
            "Trainable params: 126765 (495.18 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(padded_sequences, all_labels, epochs=10)"
      ],
      "metadata": {
        "id": "nrIhK-kabC9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ef4d77-34b5-41a1-a731-2e10f981f6b7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "27/27 [==============================] - 3s 23ms/step - loss: 1.3810 - accuracy: 0.4382\n",
            "Epoch 2/10\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 1.1377 - accuracy: 0.6152\n",
            "Epoch 3/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.8677 - accuracy: 0.7209\n",
            "Epoch 4/10\n",
            "27/27 [==============================] - 1s 36ms/step - loss: 0.7142 - accuracy: 0.7423\n",
            "Epoch 5/10\n",
            "27/27 [==============================] - 1s 36ms/step - loss: 0.5947 - accuracy: 0.8029\n",
            "Epoch 6/10\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.5250 - accuracy: 0.8325\n",
            "Epoch 7/10\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 0.4483 - accuracy: 0.8492\n",
            "Epoch 8/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.4084 - accuracy: 0.8539\n",
            "Epoch 9/10\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.3680 - accuracy: 0.8741\n",
            "Epoch 10/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.3447 - accuracy: 0.8836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(padded_sequences, all_labels, validation_data=(padded_validation, validation_labels), epochs=10)"
      ],
      "metadata": {
        "id": "mzf3mFBgMOpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "zfWznDhyI9qE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "7EtR1gk5I_4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer\n",
        "implementar incialmente exatamente como está na documentação e depois testar mudar os parâmetros"
      ],
      "metadata": {
        "id": "WP5f0Z8pJBqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "4CVoZKQfLx8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a transformer block as a layer"
      ],
      "metadata": {
        "id": "V6XmYWYjLy5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "_TudGtcWLmqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement embedding layer"
      ],
      "metadata": {
        "id": "iz14s76sL_4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "f7sGmoTIL-In"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}