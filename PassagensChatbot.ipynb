{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load data"
      ],
      "metadata": {
        "id": "MZ22pBRX98Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "8u16EHBM-rtV",
        "outputId": "17a43391-e9dc-487f-b137-9f3cba37653c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb9f88d6-2c1c-4b61-9b52-44620a99495f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cb9f88d6-2c1c-4b61-9b52-44620a99495f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dialogues_018.json to dialogues_018.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# intents\n",
        "!mkdir -p dialogue_018/NONE\n",
        "!mkdir -p dialogue_018/SearchOnewayFlight\n",
        "!mkdir -p dialogue_018/ReserveOnewayFlight\n",
        "!mkdir -p dialogue_018/SearchRoundtripFlights\n",
        "!mkdir -p dialogue_018/ReserveRoundtripFlights"
      ],
      "metadata": {
        "id": "EVVnuFYREycJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script para coletar apenas as utterances (frases) do usuário e armazená-las num diretório que representa a intenção(que será nossa classe para a tarefa de classificação de intenção).</br>\n",
        "<b>atenção</b>: O sistema foi excluído da coleta de utterances porque a resposta do sistema não possui intenção"
      ],
      "metadata": {
        "id": "mPzqHs88CbQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"dialogues_018.json\", 'r') as file:\n",
        "        data_018 = json.load(file)\n",
        "\n",
        "    for i in range(len(data_018)):\n",
        "        frames_amount = len(data_018[i]['turns'])\n",
        "        for j in range(frames_amount):\n",
        "            current_frame = data_018[i]['turns'][j]\n",
        "            speaker = current_frame['speaker']\n",
        "            if speaker == 'USER':\n",
        "                intent = current_frame['frames'][0]['state']['active_intent']\n",
        "                utterance = current_frame['utterance']\n",
        "\n",
        "                file_path = \"dialogue_018/\" + intent + \"/\" + str(i) + \"_\" + str(j) + \".txt\"\n",
        "\n",
        "                f = open(file_path, \"w+\")\n",
        "                f.write(utterance)\n",
        "                f.close()\n",
        ""
      ],
      "metadata": {
        "id": "7mD2KTnt-Rg8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Armazenar numa estrutura para treinamento no Keras"
      ],
      "metadata": {
        "id": "zAPqnY0WDLev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    '/content/dialogue_018',\n",
        "    labels='inferred',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPmrl8B1TrRt",
        "outputId": "5ec4646c-982e-4d97-e2f8-f81ea726c53a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1052 files belonging to 5 classes.\n",
            "Using 842 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre processamento de dados\n",
        "Fazer uma bag-of-words"
      ],
      "metadata": {
        "id": "Ug1zDO2rMFLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import json"
      ],
      "metadata": {
        "id": "v7s5l7y6WWlB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "\n",
        "with open(\"dialogues_018.json\", 'r') as file:\n",
        "    data_018 = json.load(file)\n",
        "\n",
        "for i in range(len(data_018)):\n",
        "    frames_amount = len(data_018[i]['turns'])\n",
        "    for j in range(frames_amount):\n",
        "        current_frame = data_018[i]['turns'][j]\n",
        "        speaker = current_frame['speaker']\n",
        "        if speaker == 'USER':\n",
        "            intent = current_frame['frames'][0]['state']['active_intent']\n",
        "            utterance = current_frame['utterance']\n",
        "\n",
        "            docs.append(utterance)"
      ],
      "metadata": {
        "id": "TBE7-hBbV0g9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWpBBYtYWdxe",
        "outputId": "205f2896-469f-461e-8e53-693dfb2702f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('i', 397), ('need', 69), ('a', 120), ('one', 27), ('way', 10), ('flight', 224), ('and', 171), ('prefer', 24), ('traveling', 33), ('in', 52), ('premium', 8), ('economy', 59), ('class', 21), ('would', 67), ('like', 74), ('to', 345), ('leave', 59), ('next', 20), ('friday', 5), ('am', 93), ('nyc', 3), ('from', 155), ('seattle', 35), ('wa', 16), ('travel', 20), ('on', 233), ('delta', 13), ('airlines', 36), ('is', 129), ('this', 89), ('refundable', 43), ('ticket', 71), ('what', 57), ('the', 358), ('arrival', 13), ('airport', 59), ('sounds', 122), ('good', 114), ('me', 195), ('yes', 122), ('can', 125), ('you', 231), ('assist', 3), ('with', 19), ('buying', 3), ('that', 195), ('correct', 20), ('all', 77), ('needed', 8), ('thank', 75), ('help', 109), ('find', 58), ('flights', 113), ('for', 264), ('my', 50), ('upcoming', 1), ('trip', 63), (\"i'll\", 24), ('be', 99), ('going', 52), ('sf', 13), ('flying', 12), ('toronto', 2), ('canada', 2), ('4th', 14), ('of', 125), ('month', 48), ('are', 54), ('any', 35), ('other', 32), ('option', 2), ('available', 29), ('there', 54), ('options', 6), ('4', 8), ('tickets', 73), ('which', 39), ('does', 103), ('it', 62), ('depart', 19), ('please', 79), ('get', 25), (\"that's\", 69), ('now', 11), ('vacation', 4), ('soon', 6), ('will', 82), ('leaving', 99), ('washington', 5), ('10th', 18), ('march', 110), ('cdmx', 4), ('business', 8), ('or', 10), ('seat', 1), ('refund', 5), ('okay', 11), ('no', 73), ('three', 9), ('actually', 5), ('only', 13), ('2', 7), ('when', 47), ('end', 1), ('at', 23), ('everything', 13), ('book', 58), ('have', 25), ('coming', 9), ('up', 5), ('1', 17), ('atlanta', 15), ('12th', 24), ('do', 20), ('where', 12), ('departing', 21), ('time', 31), ('onward', 23), ('arrive', 71), ('plan', 12), ('another', 6), ('want', 82), ('take', 5), ('so', 14), ('much', 16), ('oneway', 3), (\"i'm\", 71), ('london', 11), ('ga', 7), ('2nd', 18), ('heading', 2), ('ny', 6), ('buy', 29), ('right', 17), ('thanks', 93), ('hey', 2), ('looking', 10), ('3rd', 13), ('sd', 5), ('portland', 10), ('an', 5), ('just', 8), ('wanted', 2), ('information', 2), ('paris', 5), ('france', 2), ('8th', 19), ('some', 22), ('las', 5), ('vegas', 7), (\"don't\", 2), ('care', 2), ('about', 5), ('airline', 8), ('anything', 6), ('else', 13), ('could', 23), ('out', 8), ('phoenix', 14), ('something', 7), ('works', 13), ('new', 9), ('york', 8), ('person', 14), ('group', 18), ('planning', 14), ('traveing', 1), ('san', 25), ('diego', 4), ('also', 6), ('plans', 5), ('might', 1), ('change', 4), ('purchase', 6), ('very', 10), ('14th', 18), ('ciudad', 5), ('de', 5), ('mexico', 8), ('destination', 7), ('great', 38), ('sure', 2), ('go', 26), ('ahead', 4), ('yeah', 3), ('perfect', 26), ('nope', 8), (\"we're\", 2), ('lot', 5), ('round', 26), ('3', 10), ('people', 33), ('travelling', 4), ('philly', 2), ('thursday', 3), ('returning', 16), ('11th', 18), ('short', 28), ('behalf', 1), ('your', 54), ('look', 13), ('return', 71), ('tomorrow', 4), ('tell', 8), ('if', 13), ('has', 4), ('stop', 6), ('monday', 8), ('week', 14), (\"i'd\", 18), ('trips', 1), ('american', 13), ('la', 12), ('zero', 7), ('stops', 20), ('come', 12), ('back', 50), ('im', 2), ('atl', 9), ('fly', 14), ('saturday', 5), ('thats', 8), ('search', 40), ('los', 9), ('angeles', 9), (\"what's\", 1), ('united', 8), ('yup', 1), ('again', 4), ('9th', 17), ('vancouver', 8), ('helping', 5), ('southwest', 2), ('hometown', 1), ('5th', 14), ('attend', 1), ('family', 1), ('gathering', 1), ('sfo', 8), ('work', 7), ('oh', 1), ('nice', 1), ('but', 6), (\"you're\", 1), ('really', 2), ('big', 1), ('how', 9), ('many', 8), ('az', 7), ('ok', 9), ('landing', 1), ('chicago', 13), ('fight', 2), ('airports', 3), ('departure', 4), ('uk', 1), ('interested', 2), (\"you've\", 1), ('been', 1), ('helpful', 1), ('conference', 1), ('chi', 10), ('town', 10), ('seating', 1), (\"doesn't\", 3), ('matter', 2), ('since', 3), (\"it's\", 5), ('moving', 2), ('choice', 2), ('kindly', 1), ('persons', 3), ('whether', 1), ('0', 3), ('starts', 1), ('tuesday', 3), ('lax', 9), ('arive', 1), ('suggest', 1), ('outbound', 3), ('arrives', 5), ('francisco', 11), ('6th', 11), ('13th', 13), ('number', 1), ('two', 11), ('initial', 1), ('others', 2), ('reserve', 5), ('hello', 1), ('1st', 5), ('we', 3), ('four', 5), ('assistance', 7), ('getting', 1), ('list', 1), ('today', 9), ('because', 2), ('accept', 1), ('not', 5), ('yet', 1), ('later', 3), ('sound', 2), ('city', 6), ('too', 1), ('station', 1), ('gone', 1), ('little', 1), ('while', 1), ('through', 2), ('sorry', 2), ('see', 1), ('let', 2), ('know', 6), ('thinking', 3), ('more', 1), ('than', 1), ('exactly', 1), ('was', 1), ('alternative', 1), ('m', 1), ('may', 4), ('make', 2), ('either', 3), ('fine', 5), ('date', 2), ('7th', 10), ('schedules', 1), ('both', 2), ('locations', 1), ('looks', 1), ('think', 3), ('its', 1), ('should', 1), ('enough', 1), ('via', 1), ('as', 4), ('well', 2), ('england', 4), ('mind', 2), ('searching', 1), ('fran', 10), ('philadelphia', 7), ('form', 1), ('small', 1), ('visit', 2), (\"there's\", 2), ('mean', 1), ('upon', 1), ('alright', 3), ('able', 2), ('specifically', 1), ('delhi', 3), ('maybe', 1), ('by', 2), ('possible', 3), (\"that'll\", 1), ('wednesday', 1), ('incorrect', 1), ('total', 2), ('excellent', 1), ('check', 1), ('corrct', 1), ('wish', 1), ('ama', 1), ('outgoing', 1), ('bc', 3), ('head', 1), ('hopefully', 1), ('yep', 2), ('nairobi', 2), ('kenya', 1), ('purchasing', 2), ('sydney', 1), ('nsw', 1), ('awesome', 1), ('wonderful', 1), ('best', 1), ('us', 2), ('home', 2), ('due', 1), ('trio', 1), ('matches', 1), ('needs', 1), ('details', 2), ('run', 1), ('india', 1), ('wanting', 1), ('their', 1), ('done', 1), ('they', 2), ('usually', 1), ('shift', 1), ('anyway', 1), ('including', 1), ('myself', 1), ('names', 1), ('seats', 1), ('cancel', 1), ('taking', 3), ('use', 3), ('preferably', 1), ('off', 2), ('sunday', 2), ('urge', 1), ('accessible', 1), ('individuals', 1), ('gather', 1), ('appreciated', 2), ('offer', 1), ('require', 1), ('arriving', 3), ('nothing', 1), ('name', 1), ('enjoy', 1), ('into', 5), ('times', 3), ('purchases', 1), ('refunded', 2), ('finding', 1), ('land', 2), ('set', 1), ('them', 1), ('visiting', 2), ('making', 1), ('regret', 1), ('last', 1), ('offered', 1), ('confirm', 1), ('these', 1), ('together', 1), ('our', 1), (\"it'll\", 1), ('retun', 1), ('appreciate', 1), ('especially', 1), ('holiday', 1), ('though', 1), ('wow', 1), ('scan', 1)])\n",
            "1052\n",
            "{'i': 1, 'the': 2, 'to': 3, 'for': 4, 'on': 5, 'you': 6, 'flight': 7, 'me': 8, 'that': 9, 'and': 10, 'from': 11, 'is': 12, 'can': 13, 'of': 14, 'sounds': 15, 'yes': 16, 'a': 17, 'good': 18, 'flights': 19, 'march': 20, 'help': 21, 'does': 22, 'be': 23, 'leaving': 24, 'am': 25, 'thanks': 26, 'this': 27, 'will': 28, 'want': 29, 'please': 30, 'all': 31, 'thank': 32, 'like': 33, 'tickets': 34, 'no': 35, 'ticket': 36, 'arrive': 37, \"i'm\": 38, 'return': 39, 'need': 40, \"that's\": 41, 'would': 42, 'trip': 43, 'it': 44, 'economy': 45, 'leave': 46, 'airport': 47, 'find': 48, 'book': 49, 'what': 50, 'are': 51, 'there': 52, 'your': 53, 'in': 54, 'going': 55, 'my': 56, 'back': 57, 'month': 58, 'when': 59, 'refundable': 60, 'search': 61, 'which': 62, 'great': 63, 'airlines': 64, 'seattle': 65, 'any': 66, 'traveling': 67, 'people': 68, 'other': 69, 'time': 70, 'available': 71, 'buy': 72, 'short': 73, 'one': 74, 'go': 75, 'perfect': 76, 'round': 77, 'get': 78, 'have': 79, 'san': 80, 'prefer': 81, \"i'll\": 82, '12th': 83, 'at': 84, 'onward': 85, 'could': 86, 'some': 87, 'class': 88, 'departing': 89, 'next': 90, 'travel': 91, 'correct': 92, 'do': 93, 'stops': 94, 'with': 95, 'depart': 96, '8th': 97, '10th': 98, '2nd': 99, 'group': 100, '14th': 101, '11th': 102, \"i'd\": 103, '1': 104, 'right': 105, '9th': 106, 'wa': 107, 'much': 108, 'returning': 109, 'atlanta': 110, '4th': 111, 'so': 112, 'phoenix': 113, 'person': 114, 'planning': 115, 'week': 116, 'fly': 117, '5th': 118, 'delta': 119, 'arrival': 120, 'sf': 121, 'only': 122, 'everything': 123, '3rd': 124, 'else': 125, 'works': 126, 'look': 127, 'if': 128, 'american': 129, 'chicago': 130, '13th': 131, 'flying': 132, 'where': 133, 'plan': 134, 'la': 135, 'come': 136, 'now': 137, 'okay': 138, 'london': 139, 'francisco': 140, '6th': 141, 'two': 142, 'way': 143, 'or': 144, 'looking': 145, 'portland': 146, 'very': 147, '3': 148, 'chi': 149, 'town': 150, '7th': 151, 'fran': 152, 'three': 153, 'coming': 154, 'new': 155, 'atl': 156, 'los': 157, 'angeles': 158, 'how': 159, 'ok': 160, 'lax': 161, 'today': 162, 'premium': 163, 'needed': 164, '4': 165, 'business': 166, 'just': 167, 'airline': 168, 'out': 169, 'york': 170, 'mexico': 171, 'nope': 172, 'tell': 173, 'monday': 174, 'thats': 175, 'united': 176, 'vancouver': 177, 'sfo': 178, 'many': 179, '2': 180, 'ga': 181, 'vegas': 182, 'something': 183, 'destination': 184, 'zero': 185, 'work': 186, 'az': 187, 'assistance': 188, 'philadelphia': 189, 'options': 190, 'soon': 191, 'another': 192, 'ny': 193, 'anything': 194, 'also': 195, 'purchase': 196, 'stop': 197, 'but': 198, 'city': 199, 'know': 200, 'friday': 201, 'washington': 202, 'refund': 203, 'actually': 204, 'up': 205, 'take': 206, 'sd': 207, 'an': 208, 'paris': 209, 'las': 210, 'about': 211, 'plans': 212, 'ciudad': 213, 'de': 214, 'lot': 215, 'saturday': 216, 'helping': 217, \"it's\": 218, 'arrives': 219, 'reserve': 220, '1st': 221, 'four': 222, 'not': 223, 'fine': 224, 'into': 225, 'vacation': 226, 'cdmx': 227, 'diego': 228, 'change': 229, 'ahead': 230, 'travelling': 231, 'tomorrow': 232, 'has': 233, 'again': 234, 'departure': 235, 'may': 236, 'as': 237, 'england': 238, 'nyc': 239, 'assist': 240, 'buying': 241, 'oneway': 242, 'yeah': 243, 'thursday': 244, 'airports': 245, \"doesn't\": 246, 'since': 247, 'persons': 248, '0': 249, 'tuesday': 250, 'outbound': 251, 'we': 252, 'later': 253, 'thinking': 254, 'either': 255, 'think': 256, 'alright': 257, 'delhi': 258, 'possible': 259, 'bc': 260, 'taking': 261, 'use': 262, 'arriving': 263, 'times': 264, 'toronto': 265, 'canada': 266, 'option': 267, 'heading': 268, 'hey': 269, 'wanted': 270, 'information': 271, 'france': 272, \"don't\": 273, 'care': 274, 'sure': 275, \"we're\": 276, 'philly': 277, 'im': 278, 'southwest': 279, 'really': 280, 'fight': 281, 'interested': 282, 'matter': 283, 'moving': 284, 'choice': 285, 'others': 286, 'because': 287, 'sound': 288, 'through': 289, 'sorry': 290, 'let': 291, 'make': 292, 'date': 293, 'both': 294, 'well': 295, 'mind': 296, 'visit': 297, \"there's\": 298, 'able': 299, 'by': 300, 'total': 301, 'yep': 302, 'nairobi': 303, 'purchasing': 304, 'us': 305, 'home': 306, 'details': 307, 'they': 308, 'off': 309, 'sunday': 310, 'appreciated': 311, 'refunded': 312, 'land': 313, 'visiting': 314, 'upcoming': 315, 'seat': 316, 'end': 317, 'traveing': 318, 'might': 319, 'behalf': 320, 'trips': 321, \"what's\": 322, 'yup': 323, 'hometown': 324, 'attend': 325, 'family': 326, 'gathering': 327, 'oh': 328, 'nice': 329, \"you're\": 330, 'big': 331, 'landing': 332, 'uk': 333, \"you've\": 334, 'been': 335, 'helpful': 336, 'conference': 337, 'seating': 338, 'kindly': 339, 'whether': 340, 'starts': 341, 'arive': 342, 'suggest': 343, 'number': 344, 'initial': 345, 'hello': 346, 'getting': 347, 'list': 348, 'accept': 349, 'yet': 350, 'too': 351, 'station': 352, 'gone': 353, 'little': 354, 'while': 355, 'see': 356, 'more': 357, 'than': 358, 'exactly': 359, 'was': 360, 'alternative': 361, 'm': 362, 'schedules': 363, 'locations': 364, 'looks': 365, 'its': 366, 'should': 367, 'enough': 368, 'via': 369, 'searching': 370, 'form': 371, 'small': 372, 'mean': 373, 'upon': 374, 'specifically': 375, 'maybe': 376, \"that'll\": 377, 'wednesday': 378, 'incorrect': 379, 'excellent': 380, 'check': 381, 'corrct': 382, 'wish': 383, 'ama': 384, 'outgoing': 385, 'head': 386, 'hopefully': 387, 'kenya': 388, 'sydney': 389, 'nsw': 390, 'awesome': 391, 'wonderful': 392, 'best': 393, 'due': 394, 'trio': 395, 'matches': 396, 'needs': 397, 'run': 398, 'india': 399, 'wanting': 400, 'their': 401, 'done': 402, 'usually': 403, 'shift': 404, 'anyway': 405, 'including': 406, 'myself': 407, 'names': 408, 'seats': 409, 'cancel': 410, 'preferably': 411, 'urge': 412, 'accessible': 413, 'individuals': 414, 'gather': 415, 'offer': 416, 'require': 417, 'nothing': 418, 'name': 419, 'enjoy': 420, 'purchases': 421, 'finding': 422, 'set': 423, 'them': 424, 'making': 425, 'regret': 426, 'last': 427, 'offered': 428, 'confirm': 429, 'these': 430, 'together': 431, 'our': 432, \"it'll\": 433, 'retun': 434, 'appreciate': 435, 'especially': 436, 'holiday': 437, 'though': 438, 'wow': 439, 'scan': 440}\n",
            "defaultdict(<class 'int'>, {'and': 164, 'premium': 8, 'class': 21, 'traveling': 33, 'economy': 59, 'flight': 213, 'need': 68, 'a': 115, 'in': 52, 'way': 10, 'prefer': 24, 'one': 27, 'i': 340, 'like': 72, 'next': 20, 'friday': 5, 'to': 279, 'leave': 58, 'would': 63, 'airlines': 36, 'delta': 13, 'from': 154, 'nyc': 3, 'wa': 16, 'on': 184, 'travel': 20, 'seattle': 35, 'am': 91, 'refundable': 43, 'this': 86, 'ticket': 71, 'is': 121, 'what': 53, 'the': 306, 'arrival': 13, 'airport': 58, 'me': 184, 'sounds': 122, 'good': 114, 'with': 19, 'assist': 3, 'can': 125, 'buying': 3, 'you': 230, 'yes': 122, 'that': 190, 'correct': 20, 'all': 74, 'thank': 75, 'needed': 8, 'trip': 60, 'my': 50, 'help': 109, 'find': 58, 'flights': 113, 'for': 236, 'upcoming': 1, 'be': 93, \"i'll\": 22, 'going': 51, 'sf': 13, 'of': 110, 'month': 46, 'toronto': 2, '4th': 14, 'canada': 2, 'flying': 12, 'available': 29, 'any': 35, 'are': 52, 'other': 32, 'option': 2, '4': 8, 'there': 50, 'options': 6, 'tickets': 73, 'which': 39, 'it': 59, 'depart': 19, 'does': 93, 'please': 78, 'get': 25, \"that's\": 69, 'now': 11, 'vacation': 4, 'soon': 6, 'leaving': 96, 'washington': 5, 'cdmx': 4, 'will': 79, 'seat': 1, 'business': 8, 'or': 10, '10th': 18, 'march': 97, 'refund': 5, 'okay': 9, 'three': 9, 'no': 73, 'only': 13, 'actually': 5, '2': 7, 'end': 1, 'when': 47, 'at': 23, 'everything': 13, 'coming': 9, 'book': 57, 'up': 5, 'have': 25, '1': 17, 'atlanta': 15, '12th': 24, 'do': 19, 'arrive': 71, 'onward': 23, 'where': 12, 'time': 31, 'departing': 21, 'want': 79, 'plan': 12, 'another': 5, 'take': 5, 'so': 14, 'much': 16, 'oneway': 3, 'london': 11, \"i'm\": 68, '2nd': 18, 'ga': 7, 'ny': 6, 'heading': 2, 'buy': 29, 'right': 17, 'thanks': 93, 'looking': 10, 'hey': 2, '3rd': 13, 'sd': 5, 'an': 5, 'portland': 10, 'wanted': 2, 'just': 8, 'information': 2, 'france': 2, 'paris': 5, '8th': 19, 'some': 22, 'las': 5, 'airline': 8, \"don't\": 2, 'about': 5, 'vegas': 7, 'care': 2, 'else': 13, 'anything': 6, 'could': 23, 'out': 8, 'phoenix': 14, 'something': 7, 'works': 13, 'york': 8, 'new': 9, 'group': 18, 'planning': 14, 'person': 14, 'san': 25, 'traveing': 1, 'diego': 4, 'also': 6, 'change': 4, 'might': 1, 'plans': 5, 'purchase': 6, 'very': 10, '14th': 18, 'mexico': 8, 'ciudad': 5, 'de': 5, 'destination': 7, 'great': 38, 'sure': 2, 'ahead': 4, 'go': 26, 'yeah': 3, 'perfect': 26, 'lot': 5, \"we're\": 2, 'nope': 8, '3': 10, 'people': 33, 'travelling': 4, 'round': 26, '11th': 18, 'philly': 2, 'returning': 16, 'thursday': 3, 'behalf': 1, 'short': 28, 'your': 54, 'look': 13, 'return': 71, 'tomorrow': 4, 'tell': 8, 'stop': 6, 'if': 12, 'has': 4, 'monday': 8, \"i'd\": 18, 'week': 14, 'american': 13, 'trips': 1, 'la': 12, 'stops': 20, 'zero': 7, 'im': 2, 'come': 12, 'atl': 9, 'back': 50, 'fly': 14, 'saturday': 5, 'thats': 8, 'search': 40, 'los': 9, 'angeles': 9, \"what's\": 1, 'united': 8, 'yup': 1, 'again': 4, '9th': 17, 'vancouver': 8, 'helping': 5, 'hometown': 1, 'southwest': 2, 'gathering': 1, 'family': 1, 'attend': 1, '5th': 14, 'sfo': 8, 'work': 7, 'but': 6, 'oh': 1, 'nice': 1, \"you're\": 1, 'really': 2, 'big': 1, 'how': 9, 'many': 8, 'az': 7, 'ok': 9, 'landing': 1, 'chicago': 13, 'fight': 2, 'airports': 3, 'departure': 4, 'uk': 1, 'interested': 2, 'helpful': 1, \"you've\": 1, 'been': 1, 'conference': 1, 'town': 10, 'chi': 10, 'matter': 2, \"doesn't\": 3, 'seating': 1, \"it's\": 5, 'since': 3, 'moving': 2, 'choice': 2, 'persons': 3, 'kindly': 1, '0': 3, 'whether': 1, 'starts': 1, 'tuesday': 3, 'lax': 9, 'arive': 1, 'suggest': 1, 'outbound': 3, 'arrives': 4, 'francisco': 11, '13th': 13, '6th': 11, 'number': 1, 'two': 11, 'initial': 1, 'others': 2, 'reserve': 5, 'hello': 1, '1st': 5, 'we': 3, 'four': 5, 'assistance': 7, 'getting': 1, 'list': 1, 'today': 9, 'because': 2, 'accept': 1, 'not': 5, 'yet': 1, 'later': 3, 'sound': 2, 'city': 6, 'too': 1, 'station': 1, 'little': 1, 'while': 1, 'gone': 1, 'through': 2, 'see': 1, 'sorry': 2, 'know': 5, 'let': 2, 'thinking': 3, 'than': 1, 'more': 1, 'exactly': 1, 'was': 1, 'alternative': 1, 'm': 1, 'either': 3, 'fine': 5, 'make': 2, 'may': 4, 'date': 2, '7th': 10, 'locations': 1, 'both': 2, 'schedules': 1, 'looks': 1, 'its': 1, 'think': 3, 'enough': 1, 'should': 1, 'via': 1, 'well': 2, 'as': 3, 'england': 4, 'searching': 1, 'mind': 2, 'fran': 10, 'philadelphia': 7, 'form': 1, 'small': 1, 'visit': 2, \"there's\": 2, 'mean': 1, 'upon': 1, 'alright': 3, 'able': 2, 'specifically': 1, 'delhi': 3, 'maybe': 1, 'by': 2, 'possible': 3, \"that'll\": 1, 'wednesday': 1, 'total': 2, 'incorrect': 1, 'excellent': 1, 'check': 1, 'corrct': 1, 'wish': 1, 'ama': 1, 'outgoing': 1, 'bc': 3, 'hopefully': 1, 'head': 1, 'yep': 2, 'nairobi': 2, 'kenya': 1, 'purchasing': 2, 'sydney': 1, 'nsw': 1, 'awesome': 1, 'wonderful': 1, 'best': 1, 'us': 2, 'home': 2, 'due': 1, 'trio': 1, 'needs': 1, 'matches': 1, 'details': 2, 'run': 1, 'india': 1, 'wanting': 1, 'their': 1, 'done': 1, 'they': 2, 'usually': 1, 'anyway': 1, 'myself': 1, 'including': 1, 'shift': 1, 'names': 1, 'seats': 1, 'cancel': 1, 'taking': 3, 'use': 3, 'preferably': 1, 'off': 2, 'sunday': 2, 'urge': 1, 'individuals': 1, 'accessible': 1, 'gather': 1, 'appreciated': 2, 'require': 1, 'offer': 1, 'arriving': 3, 'nothing': 1, 'name': 1, 'enjoy': 1, 'into': 5, 'times': 3, 'purchases': 1, 'refunded': 2, 'finding': 1, 'land': 2, 'set': 1, 'them': 1, 'visiting': 2, 'making': 1, 'regret': 1, 'last': 1, 'offered': 1, 'confirm': 1, 'these': 1, 'together': 1, 'our': 1, \"it'll\": 1, 'retun': 1, 'appreciate': 1, 'especially': 1, 'holiday': 1, 'though': 1, 'wow': 1, 'scan': 1})\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 2. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 2. 2. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 2. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text, sequence"
      ],
      "metadata": {
        "id": "jxHrOBaChxa8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_token = t.texts_to_sequences(docs)\n",
        "train_data = sequence.pad_sequences(seq_token)"
      ],
      "metadata": {
        "id": "HpdgcAJ2hTvw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU7wKRXIh_XI",
        "outputId": "f35ddbd8-f4ef-4ab7-d5a0-4cd03ee945ed"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1052, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de intenções (Parte 1)\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM </br>\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/ </br>"
      ],
      "metadata": {
        "id": "Vrd0nYb4IDGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "cHp8IuBtIqbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.layers import Embedding\n",
        "import keras.backend as K\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(t.word_counts), 32))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss=['sparse_categorical_crossentropy'] , optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG-26GAEZWk8",
        "outputId": "bd492267-ef7e-49fe-a752-5ede2bb05b7f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 32)          14080     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, None, 32)          1056      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, None, 1)           33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15169 (59.25 KB)\n",
            "Trainable params: 15169 (59.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, epochs=10)"
      ],
      "metadata": {
        "id": "nrIhK-kabC9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "zfWznDhyI9qE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "7EtR1gk5I_4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer\n",
        "implementar incialmente exatamente como está na documentação e depois testar mudar os parâmetros"
      ],
      "metadata": {
        "id": "WP5f0Z8pJBqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "4CVoZKQfLx8r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a transformer block as a layer"
      ],
      "metadata": {
        "id": "V6XmYWYjLy5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "_TudGtcWLmqR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement embedding layer"
      ],
      "metadata": {
        "id": "iz14s76sL_4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "f7sGmoTIL-In"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}