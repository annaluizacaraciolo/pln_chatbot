{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ22pBRX98Rn"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "8u16EHBM-rtV",
        "outputId": "c73c0db7-d4d1-4511-98c0-49b4d9e9da68"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ec0a309-6a3a-4133-88e0-60b6d9c534a0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8ec0a309-6a3a-4133-88e0-60b6d9c534a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dialogues_018.json to dialogues_018.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EVVnuFYREycJ"
      },
      "outputs": [],
      "source": [
        "# intents\n",
        "!mkdir -p dialogue_018/NONE\n",
        "!mkdir -p dialogue_018/SearchOnewayFlight\n",
        "!mkdir -p dialogue_018/ReserveOnewayFlight\n",
        "!mkdir -p dialogue_018/SearchRoundtripFlights\n",
        "!mkdir -p dialogue_018/ReserveRoundtripFlights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPzqHs88CbQZ"
      },
      "source": [
        "Script para coletar apenas as utterances (frases) do usuário e armazená-las num diretório que representa a intenção(que será nossa classe para a tarefa de classificação de intenção).</br>\n",
        "<b>atenção</b>: O sistema foi excluído da coleta de utterances porque a resposta do sistema não possui intenção"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7mD2KTnt-Rg8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"dialogues_018.json\", 'r') as file:\n",
        "        data_018 = json.load(file)\n",
        "\n",
        "    for i in range(len(data_018)):\n",
        "        frames_amount = len(data_018[i]['turns'])\n",
        "        for j in range(frames_amount):\n",
        "            current_frame = data_018[i]['turns'][j]\n",
        "            speaker = current_frame['speaker']\n",
        "            if speaker == 'USER':\n",
        "                intent = current_frame['frames'][0]['state']['active_intent']\n",
        "                utterance = current_frame['utterance']\n",
        "\n",
        "                file_path = \"dialogue_018/\" + intent + \"/\" + str(i) + \"_\" + str(j) + \".txt\"\n",
        "\n",
        "                f = open(file_path, \"w+\")\n",
        "                f.write(utterance)\n",
        "                f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAPqnY0WDLev"
      },
      "source": [
        "Armazenar numa estrutura para treinamento no Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPmrl8B1TrRt",
        "outputId": "35033b21-53e9-4ca4-d9bf-474598ab839c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1052 files belonging to 5 classes.\n",
            "Using 842 files for training.\n",
            "Found 1052 files belonging to 5 classes.\n",
            "Using 210 files for validation.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds= tf.keras.utils.text_dataset_from_directory(\n",
        "    '/content/dialogue_018',\n",
        "    labels='inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed,\n",
        "    shuffle=False)\n",
        "\n",
        "raw_validation_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    '/content/dialogue_018',\n",
        "    labels='inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed,\n",
        "    shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFgAwDctR5W",
        "outputId": "2e428870-54a8-4137-b360-7951b0e9c6ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NONE',\n",
              " 'ReserveOnewayFlight',\n",
              " 'ReserveRoundtripFlights',\n",
              " 'SearchOnewayFlight',\n",
              " 'SearchRoundtripFlights']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "raw_train_ds.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZMI9c_kFS5J"
      },
      "outputs": [],
      "source": [
        "for text_batch, label_batch in raw_train_ds:\n",
        "  print(\"Utterance: \", text_batch)\n",
        "  print(\"Label: \", label_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV7HzrCaIyWn",
        "outputId": "4da73636-3c76-4953-cfd8-5065e74695c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "print(raw_train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrVsXgbEJjjx"
      },
      "source": [
        "Acho que o Tensor acima é dividido em batches e acredito que isso adiciona uma complexidade na hora de tokenizar, pois teríamos que fazer para cada um dos tensores. Testei resolver fazer uma concatenação de todos os tensores abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1o5cP6L6JLBx"
      },
      "outputs": [],
      "source": [
        "all_text_tensor = tf.concat([text_batch for text_batch, label_batch in raw_train_ds], axis=0)\n",
        "#all_text_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9Htct7EgLYae"
      },
      "outputs": [],
      "source": [
        "all_labels = tf.concat([label_batch for text_batch, label_batch in raw_train_ds], axis=0)\n",
        "#all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S0j0NKdNMA-9"
      },
      "outputs": [],
      "source": [
        "utterances_validation_tensor = tf.concat([text_batch for text_batch, label_batch in raw_validation_ds], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zeozrI7tMGi3"
      },
      "outputs": [],
      "source": [
        "validation_labels = tf.concat([label_batch for text_batch, label_batch in raw_validation_ds], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug1zDO2rMFLk"
      },
      "source": [
        "# Pre processamento de dados\n",
        "Fazer uma bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v7s5l7y6WWlB"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import json\n",
        "import numpy\n",
        "from keras.preprocessing import text, sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZj_2SooMm8V"
      },
      "source": [
        "### Train subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oWpBBYtYWdxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8b75dd-4b67-4bf4-fb61-606ce886cbed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('no', 73), ('that', 175), ('will', 62), ('be', 70), ('all', 75), ('thank', 75), ('you', 191), (\"that's\", 68), ('thanks', 92), ('for', 217), ('your', 54), ('help', 101), ('nope', 8), (\"we're\", 1), ('good', 101), ('a', 93), ('lot', 5), ('the', 280), ('is', 114), (\"i'm\", 46), ('right', 17), ('though', 1), ('thats', 8), ('it', 54), ('i', 282), ('just', 8), ('needed', 8), ('information', 2), ('now', 11), ('again', 4), ('everything', 13), ('would', 50), ('very', 10), ('much', 16), ('do', 17), ('not', 5), ('want', 54), ('yet', 1), ('was', 1), ('okay', 11), ('this', 63), ('should', 1), ('enough', 1), ('to', 225), ('know', 4), ('maybe', 1), ('another', 6), ('time', 24), (\"that'll\", 1), ('wanted', 2), ('but', 6), ('am', 62), ('nothing', 1), ('else', 7), ('set', 1), ('yes', 122), ('can', 93), ('assist', 3), ('me', 145), ('with', 14), ('buying', 3), ('ticket', 67), ('correct', 20), ('yeah', 3), ('perfect', 24), ('great', 34), ('sure', 2), ('go', 16), ('ahead', 4), ('and', 125), ('buy', 29), ('please', 70), ('get', 18), ('economy', 42), ('class', 14), ('tickets', 67), ('need', 51), ('three', 8), ('actually', 4), ('only', 11), ('2', 6), ('when', 36), ('does', 81), ('flight', 166), ('end', 1), ('at', 17), ('which', 35), ('airport', 51), ('plan', 9), ('book', 57), ('one', 25), ('way', 10), ('trip', 60), ('take', 2), ('so', 13), ('sounds', 109), ('travel', 12), ('soon', 5), ('oneway', 3), ('leave', 44), ('from', 103), ('what', 45), ('arrive', 57), ('refundable', 35), ('like', 53), ('could', 20), ('flying', 10), ('my', 39), ('plans', 3), ('might', 1), ('change', 2), ('purchase', 6), ('onward', 15), (\"it's\", 5), ('short', 28), ('both', 2), ('flights', 79), ('yep', 2), ('into', 4), ('ok', 8), ('them', 1), ('refund', 4), ('works', 10), ('3', 8), ('people', 25), ('details', 2), ('are', 43), ('helping', 5), ('alright', 2), ('since', 2), ('planning', 11), ('on', 153), ('making', 1), ('in', 39), ('confirm', 1), ('where', 9), ('departure', 3), ('these', 1), ('tell', 8), ('our', 1), ('destination', 4), ('if', 8), ('person', 11), ('may', 3), ('there', 43), ('together', 1), ('make', 2), ('1', 13), ('4', 7), ('fly', 8), ('out', 6), ('of', 81), ('today', 5), (\"it'll\", 1), (\"i'd\", 12), ('purchasing', 2), ('depart', 14), ('also', 5), ('arrival', 13), ('appreciate', 1), ('reserve', 5), ('going', 37), ('behalf', 1), ('refunded', 2), ('especially', 1), ('return', 50), ('arriving', 3), ('assistance', 7), ('appreciated', 2), ('has', 3), ('stop', 4), ('group', 15), ('prefer', 15), ('premium', 6), ('im', 2), ('yup', 1), (\"you're\", 1), ('really', 2), ('big', 1), ('oh', 1), ('nice', 1), ('landing', 1), ('travelling', 2), ('airports', 3), (\"you've\", 1), ('been', 1), ('helpful', 1), ('chicago', 8), ('kindly', 1), ('persons', 3), ('whether', 1), ('0', 3), ('stops', 16), ('starts', 1), ('choice', 2), ('outbound', 2), ('arrives', 2), ('seattle', 25), ('four', 5), (\"i'll\", 14), ('gone', 1), ('little', 1), ('while', 1), ('sorry', 2), ('let', 1), ('see', 1), ('available', 21), ('exactly', 1), ('because', 2), ('either', 2), ('business', 5), ('or', 8), ('fine', 4), ('think', 2), ('its', 1), ('departing', 14), ('as', 4), ('well', 2), ('small', 1), (\"there's\", 1), ('mean', 1), ('we', 2), ('upon', 1), ('how', 7), ('many', 6), ('have', 19), ('incorrect', 1), ('total', 1), ('returning', 10), ('excellent', 1), ('corrct', 1), ('any', 28), ('outgoing', 1), ('atlanta', 13), ('two', 7), ('work', 5), ('an', 4), ('wonderful', 1), ('best', 1), ('airline', 6), ('leaving', 62), ('san', 14), ('francisco', 7), ('los', 5), ('angeles', 5), ('find', 39), ('trio', 1), ('matches', 1), ('needs', 1), ('done', 1), ('names', 1), ('shift', 1), ('anyway', 1), ('including', 1), ('myself', 1), ('seats', 1), ('us', 1), ('offer', 1), ('require', 1), ('enjoy', 1), ('traveling', 23), ('times', 3), ('purchases', 1), ('able', 2), ('next', 9), ('friday', 1), ('nyc', 3), ('wa', 10), ('delta', 9), ('airlines', 24), ('paris', 4), ('france', 2), ('14th', 12), ('march', 73), ('ciudad', 4), ('de', 4), ('mexico', 5), ('diego', 4), ('upcoming', 1), ('sf', 9), ('toronto', 2), ('canada', 2), ('4th', 9), ('month', 27), ('other', 26), ('option', 1), ('options', 5), ('vacation', 4), ('washington', 3), ('10th', 13), ('cdmx', 2), ('seat', 1), ('coming', 7), ('up', 5), ('12th', 20), ('london', 8), ('ga', 7), ('2nd', 12), ('heading', 2), ('ny', 4), ('hey', 2), ('looking', 6), ('3rd', 10), ('sd', 3), ('portland', 7), ('8th', 13), ('some', 15), ('las', 4), ('vegas', 6), (\"don't\", 2), ('care', 2), ('about', 5), ('anything', 4), ('phoenix', 8), ('something', 3), ('new', 5), ('york', 5), ('traveing', 1), ('saturday', 4), ('week', 6), ('zero', 5), ('come', 9), ('back', 29), ('search', 22), ('united', 6), ('7th', 3), ('11th', 10), ('lax', 4), ('5th', 9), ('look', 8), ('6th', 6), ('use', 2), ('chi', 8), ('town', 8), ('visiting', 2), ('last', 1), ('offered', 1), ('9th', 11), ('regret', 1), ('sunday', 1), ('vancouver', 4), ('thursday', 3), ('az', 3), ('american', 7), ('la', 11), ('city', 3), ('land', 1), ('13th', 4), ('england', 2), ('fight', 2), ('possible', 1), ('retun', 1), ('fran', 3), ('1st', 3), ('philadelphia', 2), ('southwest', 2), ('round', 26), ('philly', 1), ('tomorrow', 4), ('they', 1), ('interested', 2), ('holiday', 1), ('thinking', 3), ('atl', 4), ('wow', 1), ('scan', 1), ('monday', 4), ('trips', 1), (\"what's\", 1), ('hometown', 1), ('attend', 1), ('family', 1), ('gathering', 1), ('sfo', 4), ('uk', 1), ('conference', 1), ('seating', 1), (\"doesn't\", 3), ('matter', 2), ('moving', 2), ('tuesday', 1), ('suggest', 1), ('arive', 1), ('number', 1), ('others', 2), ('initial', 1), ('hello', 1), ('getting', 1), ('list', 1), ('accept', 1), ('sound', 1), ('later', 1), ('too', 1), ('station', 1), ('through', 2), ('more', 1), ('than', 1), ('alternative', 1), ('m', 1), ('date', 1), ('schedules', 1), ('locations', 1), ('looks', 1), ('via', 1), ('mind', 1), ('searching', 1), ('form', 1), ('visit', 1), ('specifically', 1), ('delhi', 1)])\n",
            "842\n",
            "{'i': 1, 'the': 2, 'to': 3, 'for': 4, 'you': 5, 'that': 6, 'flight': 7, 'on': 8, 'me': 9, 'and': 10, 'yes': 11, 'is': 12, 'sounds': 13, 'from': 14, 'help': 15, 'good': 16, 'a': 17, 'can': 18, 'thanks': 19, 'does': 20, 'of': 21, 'flights': 22, 'all': 23, 'thank': 24, 'no': 25, 'march': 26, 'be': 27, 'please': 28, \"that's\": 29, 'ticket': 30, 'tickets': 31, 'this': 32, 'will': 33, 'am': 34, 'leaving': 35, 'trip': 36, 'book': 37, 'arrive': 38, 'your': 39, 'it': 40, 'want': 41, 'like': 42, 'need': 43, 'airport': 44, 'would': 45, 'return': 46, \"i'm\": 47, 'what': 48, 'leave': 49, 'are': 50, 'there': 51, 'economy': 52, 'my': 53, 'in': 54, 'find': 55, 'going': 56, 'when': 57, 'which': 58, 'refundable': 59, 'great': 60, 'buy': 61, 'back': 62, 'short': 63, 'any': 64, 'month': 65, 'other': 66, 'round': 67, 'one': 68, 'people': 69, 'seattle': 70, 'time': 71, 'perfect': 72, 'airlines': 73, 'traveling': 74, 'search': 75, 'available': 76, 'correct': 77, 'could': 78, '12th': 79, 'have': 80, 'get': 81, 'right': 82, 'do': 83, 'at': 84, 'much': 85, 'go': 86, 'stops': 87, 'onward': 88, 'group': 89, 'prefer': 90, 'some': 91, 'with': 92, 'class': 93, 'depart': 94, \"i'll\": 95, 'departing': 96, 'san': 97, 'everything': 98, 'so': 99, '1': 100, 'arrival': 101, 'atlanta': 102, '10th': 103, '8th': 104, 'travel': 105, \"i'd\": 106, '14th': 107, '2nd': 108, 'now': 109, 'okay': 110, 'only': 111, 'planning': 112, 'person': 113, '9th': 114, 'la': 115, 'very': 116, 'way': 117, 'flying': 118, 'works': 119, 'returning': 120, 'wa': 121, '3rd': 122, '11th': 123, 'plan': 124, 'where': 125, 'next': 126, 'delta': 127, 'sf': 128, '4th': 129, 'come': 130, '5th': 131, 'nope': 132, 'thats': 133, 'just': 134, 'needed': 135, 'three': 136, 'ok': 137, '3': 138, 'tell': 139, 'if': 140, 'fly': 141, 'chicago': 142, 'or': 143, 'london': 144, 'phoenix': 145, 'look': 146, 'chi': 147, 'town': 148, 'else': 149, '4': 150, 'assistance': 151, 'how': 152, 'two': 153, 'francisco': 154, 'coming': 155, 'ga': 156, 'portland': 157, 'american': 158, 'another': 159, 'but': 160, '2': 161, 'purchase': 162, 'out': 163, 'premium': 164, 'many': 165, 'airline': 166, 'looking': 167, 'vegas': 168, 'week': 169, 'united': 170, '6th': 171, 'lot': 172, 'not': 173, 'soon': 174, \"it's\": 175, 'helping': 176, 'today': 177, 'also': 178, 'reserve': 179, 'four': 180, 'business': 181, 'work': 182, 'los': 183, 'angeles': 184, 'mexico': 185, 'options': 186, 'up': 187, 'about': 188, 'new': 189, 'york': 190, 'zero': 191, 'again': 192, 'know': 193, 'ahead': 194, 'actually': 195, 'into': 196, 'refund': 197, 'destination': 198, 'stop': 199, 'fine': 200, 'as': 201, 'an': 202, 'paris': 203, 'ciudad': 204, 'de': 205, 'diego': 206, 'vacation': 207, 'ny': 208, 'las': 209, 'anything': 210, 'saturday': 211, 'lax': 212, 'vancouver': 213, '13th': 214, 'tomorrow': 215, 'atl': 216, 'monday': 217, 'sfo': 218, 'assist': 219, 'buying': 220, 'yeah': 221, 'oneway': 222, 'plans': 223, 'departure': 224, 'may': 225, 'arriving': 226, 'has': 227, 'airports': 228, 'persons': 229, '0': 230, 'times': 231, 'nyc': 232, 'washington': 233, 'sd': 234, 'something': 235, '7th': 236, 'thursday': 237, 'az': 238, 'city': 239, 'fran': 240, '1st': 241, 'thinking': 242, \"doesn't\": 243, 'information': 244, 'wanted': 245, 'sure': 246, 'take': 247, 'change': 248, 'both': 249, 'yep': 250, 'details': 251, 'alright': 252, 'since': 253, 'make': 254, 'purchasing': 255, 'refunded': 256, 'appreciated': 257, 'im': 258, 'really': 259, 'travelling': 260, 'choice': 261, 'outbound': 262, 'arrives': 263, 'sorry': 264, 'because': 265, 'either': 266, 'think': 267, 'well': 268, 'we': 269, 'able': 270, 'france': 271, 'toronto': 272, 'canada': 273, 'cdmx': 274, 'heading': 275, 'hey': 276, \"don't\": 277, 'care': 278, 'use': 279, 'visiting': 280, 'england': 281, 'fight': 282, 'philadelphia': 283, 'southwest': 284, 'interested': 285, 'matter': 286, 'moving': 287, 'others': 288, 'through': 289, \"we're\": 290, 'though': 291, 'yet': 292, 'was': 293, 'should': 294, 'enough': 295, 'maybe': 296, \"that'll\": 297, 'nothing': 298, 'set': 299, 'end': 300, 'might': 301, 'them': 302, 'making': 303, 'confirm': 304, 'these': 305, 'our': 306, 'together': 307, \"it'll\": 308, 'appreciate': 309, 'behalf': 310, 'especially': 311, 'yup': 312, \"you're\": 313, 'big': 314, 'oh': 315, 'nice': 316, 'landing': 317, \"you've\": 318, 'been': 319, 'helpful': 320, 'kindly': 321, 'whether': 322, 'starts': 323, 'gone': 324, 'little': 325, 'while': 326, 'let': 327, 'see': 328, 'exactly': 329, 'its': 330, 'small': 331, \"there's\": 332, 'mean': 333, 'upon': 334, 'incorrect': 335, 'total': 336, 'excellent': 337, 'corrct': 338, 'outgoing': 339, 'wonderful': 340, 'best': 341, 'trio': 342, 'matches': 343, 'needs': 344, 'done': 345, 'names': 346, 'shift': 347, 'anyway': 348, 'including': 349, 'myself': 350, 'seats': 351, 'us': 352, 'offer': 353, 'require': 354, 'enjoy': 355, 'purchases': 356, 'friday': 357, 'upcoming': 358, 'option': 359, 'seat': 360, 'traveing': 361, 'last': 362, 'offered': 363, 'regret': 364, 'sunday': 365, 'land': 366, 'possible': 367, 'retun': 368, 'philly': 369, 'they': 370, 'holiday': 371, 'wow': 372, 'scan': 373, 'trips': 374, \"what's\": 375, 'hometown': 376, 'attend': 377, 'family': 378, 'gathering': 379, 'uk': 380, 'conference': 381, 'seating': 382, 'tuesday': 383, 'suggest': 384, 'arive': 385, 'number': 386, 'initial': 387, 'hello': 388, 'getting': 389, 'list': 390, 'accept': 391, 'sound': 392, 'later': 393, 'too': 394, 'station': 395, 'more': 396, 'than': 397, 'alternative': 398, 'm': 399, 'date': 400, 'schedules': 401, 'locations': 402, 'looks': 403, 'via': 404, 'mind': 405, 'searching': 406, 'form': 407, 'visit': 408, 'specifically': 409, 'delhi': 410}\n",
            "defaultdict(<class 'int'>, {'all': 72, 'no': 73, 'be': 68, 'that': 170, 'will': 61, 'thank': 75, 'you': 190, \"that's\": 68, 'your': 54, 'thanks': 92, 'help': 101, 'for': 198, 'a': 90, 'good': 101, \"we're\": 1, 'nope': 8, 'the': 240, 'lot': 5, 'is': 106, \"i'm\": 44, 'right': 17, 'though': 1, 'thats': 8, 'it': 51, 'i': 248, 'needed': 8, 'just': 8, 'information': 2, 'now': 11, 'again': 4, 'everything': 13, 'would': 48, 'much': 16, 'very': 10, 'yet': 1, 'want': 51, 'not': 5, 'do': 16, 'was': 1, 'okay': 9, 'this': 60, 'should': 1, 'enough': 1, 'to': 186, 'know': 3, 'maybe': 1, 'time': 24, 'another': 5, \"that'll\": 1, 'wanted': 2, 'but': 6, 'am': 61, 'nothing': 1, 'else': 7, 'set': 1, 'assist': 3, 'with': 14, 'yes': 122, 'ticket': 67, 'can': 93, 'me': 134, 'buying': 3, 'correct': 20, 'yeah': 3, 'perfect': 24, 'great': 34, 'go': 16, 'ahead': 4, 'and': 118, 'buy': 29, 'sure': 2, 'please': 69, 'tickets': 67, 'class': 14, 'economy': 42, 'get': 18, 'need': 51, 'three': 8, '2': 6, 'only': 11, 'actually': 4, 'does': 74, 'which': 35, 'at': 17, 'airport': 51, 'end': 1, 'flight': 159, 'when': 36, 'way': 10, 'take': 2, 'book': 56, 'trip': 57, 'one': 25, 'plan': 9, 'so': 13, 'oneway': 3, 'travel': 12, 'soon': 5, 'sounds': 109, 'leave': 43, 'from': 103, 'what': 43, 'arrive': 57, 'refundable': 35, 'could': 20, 'like': 52, 'flying': 10, 'plans': 3, 'change': 2, 'my': 39, 'might': 1, 'purchase': 6, 'onward': 15, 'short': 28, \"it's\": 5, 'both': 2, 'flights': 79, 'into': 4, 'yep': 2, 'ok': 8, 'them': 1, 'refund': 4, 'works': 10, 'people': 25, '3': 8, 'are': 42, 'details': 2, 'helping': 5, 'making': 1, 'alright': 2, 'planning': 11, 'since': 2, 'on': 120, 'in': 39, 'confirm': 1, 'where': 9, 'departure': 3, 'these': 1, 'tell': 8, 'our': 1, 'destination': 4, 'if': 7, 'may': 3, 'person': 11, 'there': 40, 'together': 1, '1': 13, 'make': 2, '4': 7, 'fly': 8, 'out': 6, 'of': 72, 'today': 5, 'purchasing': 2, \"it'll\": 1, \"i'd\": 12, 'depart': 14, 'arrival': 13, 'also': 5, 'appreciate': 1, 'reserve': 5, 'behalf': 1, 'going': 37, 'refunded': 2, 'especially': 1, 'return': 50, 'arriving': 3, 'assistance': 7, 'appreciated': 2, 'has': 3, 'stop': 4, 'prefer': 15, 'group': 15, 'premium': 6, 'im': 2, 'yup': 1, \"you're\": 1, 'really': 2, 'big': 1, 'nice': 1, 'oh': 1, 'landing': 1, 'travelling': 2, 'airports': 3, \"you've\": 1, 'helpful': 1, 'been': 1, 'chicago': 8, 'kindly': 1, 'persons': 3, 'stops': 16, 'whether': 1, '0': 3, 'starts': 1, 'choice': 2, 'outbound': 2, 'arrives': 2, 'seattle': 25, 'four': 5, 'gone': 1, \"i'll\": 12, 'little': 1, 'while': 1, 'sorry': 2, 'let': 1, 'available': 21, 'see': 1, 'exactly': 1, 'business': 5, 'fine': 4, 'or': 8, 'because': 2, 'either': 2, 'its': 1, 'think': 2, 'well': 2, 'departing': 14, 'as': 3, 'small': 1, \"there's\": 1, 'mean': 1, 'upon': 1, 'we': 2, 'how': 7, 'have': 19, 'many': 6, 'total': 1, 'incorrect': 1, 'returning': 10, 'excellent': 1, 'corrct': 1, 'any': 28, 'outgoing': 1, 'atlanta': 13, 'two': 7, 'work': 5, 'an': 4, 'wonderful': 1, 'best': 1, 'airline': 6, 'leaving': 62, 'san': 14, 'francisco': 7, 'angeles': 5, 'los': 5, 'find': 39, 'trio': 1, 'needs': 1, 'matches': 1, 'done': 1, 'names': 1, 'including': 1, 'shift': 1, 'anyway': 1, 'myself': 1, 'seats': 1, 'us': 1, 'require': 1, 'offer': 1, 'enjoy': 1, 'traveling': 23, 'times': 3, 'purchases': 1, 'able': 2, 'next': 9, 'friday': 1, 'wa': 10, 'nyc': 3, 'delta': 9, 'airlines': 24, 'paris': 4, 'france': 2, 'march': 63, '14th': 12, 'mexico': 5, 'de': 4, 'diego': 4, 'ciudad': 4, 'upcoming': 1, 'sf': 9, '4th': 9, 'month': 25, 'toronto': 2, 'canada': 2, 'option': 1, 'other': 26, 'options': 5, 'vacation': 4, 'seat': 1, 'washington': 3, '10th': 13, 'cdmx': 2, 'coming': 7, 'up': 5, '12th': 20, 'london': 8, '2nd': 12, 'ga': 7, 'ny': 4, 'heading': 2, 'looking': 6, '3rd': 10, 'hey': 2, 'sd': 3, 'portland': 7, '8th': 13, 'some': 15, 'vegas': 6, \"don't\": 2, 'about': 5, 'las': 4, 'care': 2, 'anything': 4, 'phoenix': 8, 'something': 3, 'new': 5, 'york': 5, 'traveing': 1, 'saturday': 4, 'week': 6, 'zero': 5, 'come': 9, 'back': 29, 'united': 6, 'search': 22, '7th': 3, '11th': 10, 'lax': 4, '5th': 9, 'look': 8, '6th': 6, 'town': 8, 'use': 2, 'chi': 8, 'visiting': 2, 'last': 1, 'offered': 1, '9th': 11, 'regret': 1, 'sunday': 1, 'vancouver': 4, 'thursday': 3, 'az': 3, 'american': 7, 'la': 11, 'city': 3, 'land': 1, '13th': 4, 'england': 2, 'possible': 1, 'fight': 2, 'retun': 1, 'fran': 3, '1st': 3, 'philadelphia': 2, 'southwest': 2, 'round': 26, 'philly': 1, 'tomorrow': 4, 'they': 1, 'holiday': 1, 'interested': 2, 'thinking': 3, 'atl': 4, 'wow': 1, 'scan': 1, 'monday': 4, 'trips': 1, \"what's\": 1, 'hometown': 1, 'attend': 1, 'sfo': 4, 'gathering': 1, 'family': 1, 'uk': 1, 'conference': 1, 'seating': 1, 'matter': 2, \"doesn't\": 3, 'moving': 2, 'tuesday': 1, 'suggest': 1, 'arive': 1, 'number': 1, 'others': 2, 'initial': 1, 'hello': 1, 'getting': 1, 'list': 1, 'accept': 1, 'sound': 1, 'later': 1, 'station': 1, 'too': 1, 'through': 2, 'more': 1, 'than': 1, 'alternative': 1, 'm': 1, 'date': 1, 'schedules': 1, 'locations': 1, 'looks': 1, 'via': 1, 'mind': 1, 'searching': 1, 'form': 1, 'visit': 1, 'specifically': 1, 'delhi': 1})\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 2. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "t = Tokenizer()\n",
        "texts = all_text_tensor.numpy().astype(str)\n",
        "t.fit_on_texts(texts)\n",
        "\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(texts, mode='count')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HpdgcAJ2hTvw"
      },
      "outputs": [],
      "source": [
        "seq_token = t.texts_to_sequences(texts)\n",
        "padded_sequences = sequence.pad_sequences(seq_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU7wKRXIh_XI",
        "outputId": "2266f830-9d30-440d-bd68-a418c864dbc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(842, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "padded_sequences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do62_JxXN6s3",
        "outputId": "56645961-f643-4b9d-9705-b0e1a94d9ec1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([842])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "all_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EYJ9PZOMq-w"
      },
      "source": [
        "### Validation subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xWtIC13BMtHB"
      },
      "outputs": [],
      "source": [
        "t2 = Tokenizer()\n",
        "validation_texts = utterances_validation_tensor.numpy().astype(str)\n",
        "t2.fit_on_texts(validation_texts)\n",
        "validation_seq_token = t2.texts_to_sequences(validation_texts)\n",
        "padded_validation = sequence.pad_sequences(validation_seq_token)\n",
        "#padded_validation = sequence.pad_sequences(validation_seq_token, maxlen=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlKhpjirNkUA",
        "outputId": "1c78f587-ea95-4048-ef1c-1e2c804a6c4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "padded_validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Xna25DNty1",
        "outputId": "ff9e6a89-4733-400f-a3ce-ad0cd35e08f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([210])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "validation_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrd0nYb4IDGn"
      },
      "source": [
        "# Classificação de intenções (Parte 1)\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM </br>\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/ </br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHp8IuBtIqbn"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG-26GAEZWk8",
        "outputId": "f37634c6-9804-4270-d7f2-21f210a65c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_13 (Embedding)    (None, None, 100)         84200     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                42240     \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 126765 (495.18 KB)\n",
            "Trainable params: 126765 (495.18 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM\n",
        "from keras.layers import Embedding\n",
        "import keras.backend as K\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim=842, output_dim=100))\n",
        "model_lstm.add(LSTM(units=64))\n",
        "model_lstm.add(Dense(5,activation='softmax'))\n",
        "model_lstm.compile(loss=['sparse_categorical_crossentropy'] , optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrIhK-kabC9H",
        "outputId": "ce7cb2b7-a9b8-4f15-e345-bbd377d3acc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "27/27 [==============================] - 3s 22ms/step - loss: 1.3791 - accuracy: 0.4513\n",
            "Epoch 2/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 1.1015 - accuracy: 0.6306\n",
            "Epoch 3/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.8571 - accuracy: 0.7067\n",
            "Epoch 4/10\n",
            "27/27 [==============================] - 1s 24ms/step - loss: 0.6919 - accuracy: 0.7494\n",
            "Epoch 5/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.5871 - accuracy: 0.7933\n",
            "Epoch 6/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.5115 - accuracy: 0.8349\n",
            "Epoch 7/10\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.4486 - accuracy: 0.8551\n",
            "Epoch 8/10\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.4092 - accuracy: 0.8504\n",
            "Epoch 9/10\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.3609 - accuracy: 0.8741\n",
            "Epoch 10/10\n",
            "27/27 [==============================] - 1s 39ms/step - loss: 0.3231 - accuracy: 0.8931\n"
          ]
        }
      ],
      "source": [
        "history_lstm = model_lstm.fit(padded_sequences, all_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzf3mFBgMOpB",
        "outputId": "f9e0d544-905a-47f7-b9fd-5c1c43f94add"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "27/27 [==============================] - 1s 48ms/step - loss: 0.2933 - accuracy: 0.9050 - val_loss: 2.8053 - val_accuracy: 0.2952\n",
            "Epoch 2/10\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.2887 - accuracy: 0.9026 - val_loss: 2.6944 - val_accuracy: 0.3143\n",
            "Epoch 3/10\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.2601 - accuracy: 0.9169 - val_loss: 2.4873 - val_accuracy: 0.3381\n",
            "Epoch 4/10\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.2324 - accuracy: 0.9216 - val_loss: 2.0444 - val_accuracy: 0.4762\n",
            "Epoch 5/10\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.2200 - accuracy: 0.9287 - val_loss: 2.4590 - val_accuracy: 0.3952\n",
            "Epoch 6/10\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.2256 - accuracy: 0.9169 - val_loss: 2.4315 - val_accuracy: 0.3714\n",
            "Epoch 7/10\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.2115 - accuracy: 0.9264 - val_loss: 2.9085 - val_accuracy: 0.3190\n",
            "Epoch 8/10\n",
            "27/27 [==============================] - 1s 44ms/step - loss: 0.2036 - accuracy: 0.9276 - val_loss: 3.2231 - val_accuracy: 0.3095\n",
            "Epoch 9/10\n",
            "27/27 [==============================] - 1s 45ms/step - loss: 0.1971 - accuracy: 0.9311 - val_loss: 2.9379 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.1865 - accuracy: 0.9335 - val_loss: 3.3628 - val_accuracy: 0.3286\n"
          ]
        }
      ],
      "source": [
        "history_with_validation_lstm = model_lstm.fit(padded_sequences, all_labels, validation_data=(padded_validation, validation_labels), epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfWznDhyI9qE"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "with open(\"dialogues_018.json\", 'r') as file:\n",
        "    data_018 = json.load(file)\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(data_018)):\n",
        "    frames_amount = len(data_018[i]['turns'])\n",
        "    for j in range(frames_amount):\n",
        "        current_frame = data_018[i]['turns'][j]\n",
        "        speaker = current_frame['speaker']\n",
        "        if speaker == 'USER':\n",
        "            intent = current_frame['frames'][0]['state']['active_intent']\n",
        "            utterance = current_frame['utterance']\n",
        "            texts.append(utterance)\n",
        "            labels.append(intent)\n",
        "\n",
        "texts_train, texts_validation, labels_train, labels_validation = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(texts_train)\n",
        "X_validation = vectorizer.transform(texts_validation)\n",
        "\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, labels_train)\n",
        "\n",
        "predictions = svm_model.predict(X_validation)\n",
        "\n",
        "accuracy = accuracy_score(labels_validation, predictions)\n",
        "print(f'Acurácia do modelo SVM: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNy6bwDU8j3B",
        "outputId": "1018d849-02f6-4d7c-d2b8-1daf9e29c6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo SVM: 0.8104265402843602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de diálogo\n",
        "example_dialogue = [\n",
        "    {\"speaker\": \"USER\", \"utterance\": \"I would like a short trip to New York\"}\n",
        "    # Adicione mais turnos do diálogo conforme necessário\n",
        "]\n",
        "\n",
        "# Pré-processamento do exemplo de diálogo\n",
        "example_text = example_dialogue[0]['utterance']\n",
        "example_vector = vectorizer.transform([example_text])\n",
        "\n",
        "# Previsão do modelo SVM\n",
        "predicted_intent = svm_model.predict(example_vector)\n",
        "\n",
        "# Resultados\n",
        "print(f\"Texto do Usuário: {example_text}\")\n",
        "print(f\"Intenção Prevista: {predicted_intent[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVX4CJlF-d2r",
        "outputId": "c3284575-5c5a-4cfa-891d-340bef9d8862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto do Usuário: I would like a short trip to New York\n",
            "Intenção Prevista: SearchRoundtripFlights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EtR1gk5I_4A"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JToZVtBNKY24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=842, output_dim=100))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Alterado para categorical_crossentropy\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Convertendo os rótulos para codificação one-hot\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "labels_one_hot = to_categorical(all_labels, num_classes=5)\n",
        "\n",
        "# Fit do modelo\n",
        "history = model.fit(padded_sequences, labels_one_hot, epochs=10, validation_data=(padded_validation, to_categorical(validation_labels, num_classes=5)), batch_size=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQFSwl2lODBX",
        "outputId": "c89296a0-b2be-4663-df84-d0d50a769a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, None, 100)         84200     \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, None, 128)         64128     \n",
            "                                                                 \n",
            " global_max_pooling1d_8 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 156909 (612.93 KB)\n",
            "Trainable params: 156909 (612.93 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "169/169 [==============================] - 6s 28ms/step - loss: 0.9865 - accuracy: 0.6496 - val_loss: 1.3784 - val_accuracy: 0.4571\n",
            "Epoch 2/10\n",
            "169/169 [==============================] - 1s 9ms/step - loss: 0.5180 - accuracy: 0.8325 - val_loss: 1.8332 - val_accuracy: 0.4333\n",
            "Epoch 3/10\n",
            "169/169 [==============================] - 1s 6ms/step - loss: 0.3749 - accuracy: 0.8717 - val_loss: 2.7881 - val_accuracy: 0.2571\n",
            "Epoch 4/10\n",
            "169/169 [==============================] - 1s 6ms/step - loss: 0.2939 - accuracy: 0.8907 - val_loss: 1.9813 - val_accuracy: 0.4667\n",
            "Epoch 5/10\n",
            "169/169 [==============================] - 1s 7ms/step - loss: 0.2171 - accuracy: 0.9240 - val_loss: 2.2794 - val_accuracy: 0.4476\n",
            "Epoch 6/10\n",
            "169/169 [==============================] - 1s 5ms/step - loss: 0.1757 - accuracy: 0.9371 - val_loss: 2.6466 - val_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "169/169 [==============================] - 1s 5ms/step - loss: 0.1445 - accuracy: 0.9477 - val_loss: 2.5176 - val_accuracy: 0.4286\n",
            "Epoch 8/10\n",
            "169/169 [==============================] - 1s 5ms/step - loss: 0.1246 - accuracy: 0.9525 - val_loss: 2.7134 - val_accuracy: 0.4667\n",
            "Epoch 9/10\n",
            "169/169 [==============================] - 1s 5ms/step - loss: 0.1086 - accuracy: 0.9596 - val_loss: 3.5510 - val_accuracy: 0.3095\n",
            "Epoch 10/10\n",
            "169/169 [==============================] - 1s 5ms/step - loss: 0.1021 - accuracy: 0.9572 - val_loss: 4.1802 - val_accuracy: 0.2714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supondo que você tenha um novo texto a ser classificado\n",
        "new_text = \"i need to buy a one way flight\"\n",
        "\n",
        "new_sequence = t.texts_to_sequences([new_text])\n",
        "padded_new_sequence = pad_sequences(new_sequence, maxlen=10, padding='post')\n",
        "\n",
        "predictions = model.predict(padded_new_sequence)\n",
        "\n",
        "predicted_class = predictions.argmax(axis=-1)\n",
        "\n",
        "print(f'O texto foi classificado como classe: {predicted_class[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91BRkozEPtpF",
        "outputId": "c7934442-b528-49aa-9d23-4463138da374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 115ms/step\n",
            "O texto foi classificado como classe: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP5f0Z8pJBqi"
      },
      "source": [
        "## Transformer\n",
        "implementar incialmente exatamente como está na documentação e depois testar mudar os parâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G75RfMxsHgcO"
      },
      "source": [
        "### Teste de transformer com BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BusOX61MHjt4"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_NIt4xuH4aU",
        "outputId": "1cb26ad9-6133-4a65-e38d-beb3572a6c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load BERT tokenizer and model\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "v0Q_FlMyMhsT"
      },
      "outputs": [],
      "source": [
        "# Modify the model's classifier layer\n",
        "classifier_layer = Dense(5, activation='softmax',  name='classifier')\n",
        "bert_model.layers[-1]= classifier_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zc16HSmWJXsR"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "opt = keras.optimizers.Adam(learning_rate=2e-5)\n",
        "bert_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_E03K1sJpWX",
        "outputId": "0e5ae41c-eba8-43c0-cea6-6dd86e2ed5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "27/27 [==============================] - 328s 11s/step - loss: 3.1591 - accuracy: 0.3753\n",
            "Epoch 2/3\n",
            "27/27 [==============================] - 279s 10s/step - loss: 1.9506 - accuracy: 0.3717\n",
            "Epoch 3/3\n",
            "27/27 [==============================] - 283s 10s/step - loss: 1.4578 - accuracy: 0.4204\n"
          ]
        }
      ],
      "source": [
        "bert_history = bert_model.fit(\n",
        "    padded_sequences,\n",
        "    all_labels,\n",
        "    epochs=3\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cHp8IuBtIqbn"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}